12:55:59,7 graphrag.cli.index INFO Logging enabled at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\logs\indexing-engine.log
12:55:59,11 graphrag.cli.index INFO Starting pipeline run for: 20241210-125558, dry_run=False
12:55:59,12 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "glm-4-air",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://open.bigmodel.cn/api/paas/v4",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": null,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "embedding-3",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
12:55:59,20 graphrag.index.create_pipeline_config INFO skipping workflows 
12:55:59,20 graphrag.index.run.run INFO Running pipeline
12:55:59,20 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\output
12:55:59,21 graphrag.index.input.load_input INFO loading input from root_dir=input
12:55:59,21 graphrag.index.input.load_input INFO using file storage for input
12:55:59,23 graphrag.index.storage.file_pipeline_storage INFO search D:\GISERR\graduate_study\pytorch\graphrag\ragtest\input for files matching .*\.txt$
12:55:59,24 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
12:55:59,29 graphrag.index.input.text INFO Found 1 files, loading 1
12:55:59,34 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
12:55:59,35 graphrag.index.run.run INFO Final # of rows loaded: 1
12:55:59,195 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
12:55:59,201 datashaper.workflow.workflow INFO executing verb create_base_text_units
12:56:02,925 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
12:56:02,926 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
12:56:02,933 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
12:56:02,942 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
12:56:03,383 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4-air: TPM=0, RPM=0
12:56:03,383 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4-air: 25
12:56:06,217 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:06,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.780999999999949. input_tokens=2935, output_tokens=51
12:56:06,337 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:06,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.844000000000051. input_tokens=2936, output_tokens=59
12:56:06,357 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:06,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.875. input_tokens=2936, output_tokens=59
12:56:06,671 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:06,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.217999999999847. input_tokens=2936, output_tokens=72
12:56:06,959 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:06,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.54700000000048. input_tokens=2936, output_tokens=57
12:56:06,984 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:06,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.4840000000003783. input_tokens=2936, output_tokens=80
12:56:07,567 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:07,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.07799999999952. input_tokens=2936, output_tokens=94
12:56:07,795 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:07,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.342999999999847. input_tokens=2936, output_tokens=70
12:56:07,816 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:07,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.390999999999622. input_tokens=2936, output_tokens=89
12:56:08,276 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:08,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.765999999999622. input_tokens=2936, output_tokens=93
12:56:08,620 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:08,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.171999999999571. input_tokens=2936, output_tokens=59
12:56:08,626 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:08,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.171999999999571. input_tokens=2936, output_tokens=123
12:56:08,744 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:08,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.25. input_tokens=2936, output_tokens=71
12:56:08,927 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:08,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.453000000000429. input_tokens=2936, output_tokens=91
12:56:10,94 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:10,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.655999999999949. input_tokens=2936, output_tokens=59
12:56:11,89 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:11,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.655999999999949. input_tokens=2936, output_tokens=115
12:56:11,334 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:11,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.04700000000048. input_tokens=2937, output_tokens=62
12:56:11,418 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:11,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.953000000000429. input_tokens=2936, output_tokens=89
12:56:11,542 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:11,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7960000000002765. input_tokens=2936, output_tokens=52
12:56:11,908 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:11,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.094000000000051. input_tokens=2935, output_tokens=103
12:56:11,996 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:11,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.640999999999622. input_tokens=2936, output_tokens=90
12:56:12,13 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:12,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.219000000000051. input_tokens=2935, output_tokens=52
12:56:12,188 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:12,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.844000000000051. input_tokens=2935, output_tokens=89
12:56:12,266 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:12,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.75. input_tokens=2936, output_tokens=80
12:56:12,487 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:12,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.563000000000102. input_tokens=2936, output_tokens=59
12:56:12,835 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:12,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.266000000000531. input_tokens=2936, output_tokens=54
12:56:13,214 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:13,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.592999999999847. input_tokens=2936, output_tokens=76
12:56:13,791 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:13,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.842999999999847. input_tokens=2934, output_tokens=120
12:56:14,926 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:14,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.70300000000043. input_tokens=2936, output_tokens=66
12:56:15,630 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:15,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.640999999999622. input_tokens=2936, output_tokens=102
12:56:17,698 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:17,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.29700000000048. input_tokens=2935, output_tokens=357
12:56:19,839 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:19,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.217999999999847. input_tokens=2936, output_tokens=293
12:56:21,583 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:21,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.907000000000153. input_tokens=2934, output_tokens=474
12:56:23,492 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:23,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.0. input_tokens=2935, output_tokens=553
12:56:24,578 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:24,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.094000000000051. input_tokens=34, output_tokens=327
12:56:26,4 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:26,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.907000000000153. input_tokens=2937, output_tokens=373
12:56:27,936 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:27,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.51599999999962. input_tokens=2935, output_tokens=540
12:56:28,708 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:28,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.875. input_tokens=34, output_tokens=385
12:56:29,617 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:29,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.766000000000531. input_tokens=34, output_tokens=285
12:56:30,562 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:30,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.65599999999995. input_tokens=34, output_tokens=433
12:56:31,894 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:31,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.46900000000005. input_tokens=2937, output_tokens=539
12:56:32,173 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:32,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.90599999999995. input_tokens=34, output_tokens=313
12:56:32,258 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:32,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.234999999999673. input_tokens=34, output_tokens=392
12:56:32,829 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:32,833 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.032000000000153. input_tokens=34, output_tokens=404
12:56:33,67 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:33,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.061999999999898. input_tokens=34, output_tokens=415
12:56:33,450 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:33,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.938000000000102. input_tokens=2935, output_tokens=897
12:56:33,583 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:33,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.0. input_tokens=34, output_tokens=360
12:56:33,688 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:33,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.20299999999952. input_tokens=34, output_tokens=377
12:56:34,325 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:34,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.625. input_tokens=34, output_tokens=494
12:56:34,703 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:34,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.157000000000153. input_tokens=34, output_tokens=464
12:56:34,711 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:34,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.51600000000053. input_tokens=34, output_tokens=509
12:56:35,315 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:35,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.811999999999898. input_tokens=2936, output_tokens=622
12:56:37,974 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:37,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.342999999999847. input_tokens=34, output_tokens=494
12:56:39,159 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:39,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.938000000000102. input_tokens=34, output_tokens=520
12:56:39,215 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:39,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.889999999999418. input_tokens=2790, output_tokens=767
12:56:39,736 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:39,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.64100000000053. input_tokens=2935, output_tokens=488
12:56:40,173 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:40,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.75. input_tokens=34, output_tokens=665
12:56:41,946 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:41,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.108999999999469. input_tokens=34, output_tokens=207
12:56:43,177 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:43,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.25. input_tokens=34, output_tokens=705
12:56:44,851 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:44,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.375. input_tokens=2935, output_tokens=914
12:56:46,644 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:46,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.70300000000043. input_tokens=34, output_tokens=532
12:56:47,321 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:47,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.73399999999947. input_tokens=34, output_tokens=797
12:56:49,637 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:49,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.936999999999898. input_tokens=34, output_tokens=300
12:56:49,700 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:49,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.25. input_tokens=34, output_tokens=521
12:56:50,530 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:50,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.82799999999952. input_tokens=34, output_tokens=424
12:56:51,79 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:51,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.01600000000053. input_tokens=34, output_tokens=457
12:56:51,290 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:51,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.969000000000051. input_tokens=34, output_tokens=427
12:56:51,911 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:51,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.734999999999673. input_tokens=34, output_tokens=337
12:56:52,97 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:52,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.375. input_tokens=34, output_tokens=354
12:56:54,569 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:54,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.561999999999898. input_tokens=34, output_tokens=693
12:56:55,152 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:55,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.82799999999952. input_tokens=34, output_tokens=395
12:56:55,793 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:55,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.061999999999898. input_tokens=34, output_tokens=414
12:56:56,497 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:56,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.89099999999962. input_tokens=34, output_tokens=643
12:56:56,908 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:56,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.75. input_tokens=34, output_tokens=476
12:56:57,687 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:57,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.71900000000005. input_tokens=34, output_tokens=563
12:56:58,798 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:58,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.546000000000276. input_tokens=34, output_tokens=508
12:56:59,138 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:56:59,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.57800000000043. input_tokens=34, output_tokens=606
12:57:00,294 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:00,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.07800000000043. input_tokens=34, output_tokens=510
12:57:00,708 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:00,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.01600000000053. input_tokens=34, output_tokens=740
12:57:01,170 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:01,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.311999999999898. input_tokens=34, output_tokens=506
12:57:02,329 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:02,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.438000000000102. input_tokens=34, output_tokens=704
12:57:02,908 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:02,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.96900000000005. input_tokens=34, output_tokens=513
12:57:03,398 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:03,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.21900000000005. input_tokens=34, output_tokens=611
12:57:05,864 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:05,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.28099999999995. input_tokens=34, output_tokens=656
12:57:07,966 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:07,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9679999999998472. input_tokens=149, output_tokens=36
12:57:08,204 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.188000000000102. input_tokens=165, output_tokens=68
12:57:08,229 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.25. input_tokens=171, output_tokens=74
12:57:08,281 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.313000000000102. input_tokens=156, output_tokens=61
12:57:08,513 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.561999999999898. input_tokens=173, output_tokens=70
12:57:08,588 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.639999999999418. input_tokens=208, output_tokens=70
12:57:08,605 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.594000000000051. input_tokens=175, output_tokens=59
12:57:08,663 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6409999999996217. input_tokens=172, output_tokens=61
12:57:08,934 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9219999999995707. input_tokens=159, output_tokens=56
12:57:08,991 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:08,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9530000000004293. input_tokens=169, output_tokens=56
12:57:09,150 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:09,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.188000000000102. input_tokens=220, output_tokens=114
12:57:09,443 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:09,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.469000000000051. input_tokens=174, output_tokens=89
12:57:09,486 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:09,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.25. input_tokens=149, output_tokens=37
12:57:09,808 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:09,809 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:09,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.780999999999949. input_tokens=208, output_tokens=87
12:57:09,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.219000000000051. input_tokens=149, output_tokens=30
12:57:10,49 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,50 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.092999999999847. input_tokens=180, output_tokens=93
12:57:10,101 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.125. input_tokens=205, output_tokens=92
12:57:10,162 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5. input_tokens=157, output_tokens=46
12:57:10,533 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,535 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.515999999999622. input_tokens=175, output_tokens=74
12:57:10,571 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.530999999999949. input_tokens=167, output_tokens=70
12:57:10,657 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.219000000000051. input_tokens=151, output_tokens=30
12:57:10,757 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.75. input_tokens=338, output_tokens=138
12:57:10,961 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:10,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.016000000000531. input_tokens=154, output_tokens=66
12:57:11,57 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:11,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.07799999999952. input_tokens=181, output_tokens=90
12:57:11,104 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:11,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.594000000000051. input_tokens=169, output_tokens=73
12:57:11,276 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:11,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6719999999995707. input_tokens=155, output_tokens=61
12:57:11,359 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:11,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.375. input_tokens=192, output_tokens=55
12:57:11,523 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:11,525 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.515000000000327. input_tokens=176, output_tokens=85
12:57:11,737 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:11,738 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.75. input_tokens=309, output_tokens=187
12:57:12,28 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:12,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5469999999995707. input_tokens=170, output_tokens=61
12:57:12,104 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:12,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9530000000004293. input_tokens=171, output_tokens=74
12:57:12,543 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:12,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5. input_tokens=176, output_tokens=60
12:57:12,650 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:12,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8280000000004293. input_tokens=215, output_tokens=87
12:57:12,719 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:12,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.436999999999898. input_tokens=169, output_tokens=97
12:57:12,981 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:12,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.42200000000048. input_tokens=169, output_tokens=45
12:57:13,17 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:13,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.92200000000048. input_tokens=165, output_tokens=47
12:57:13,214 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:13,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.061999999999898. input_tokens=173, output_tokens=63
12:57:13,513 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:13,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8590000000003783. input_tokens=170, output_tokens=73
12:57:13,585 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:13,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.578000000000429. input_tokens=189, output_tokens=90
12:57:13,788 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:13,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.813000000000102. input_tokens=272, output_tokens=161
12:57:14,50 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9840000000003783. input_tokens=197, output_tokens=101
12:57:14,93 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9839999999994689. input_tokens=155, output_tokens=31
12:57:14,96 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.342999999999847. input_tokens=178, output_tokens=76
12:57:14,198 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.92200000000048. input_tokens=168, output_tokens=60
12:57:14,446 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.405999999999949. input_tokens=194, output_tokens=75
12:57:14,618 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.078000000000429. input_tokens=204, output_tokens=134
12:57:14,640 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.686999999999898. input_tokens=304, output_tokens=185
12:57:14,811 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.70299999999952. input_tokens=249, output_tokens=110
12:57:14,860 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:14,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.141000000000531. input_tokens=171, output_tokens=58
12:57:15,84 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:15,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.875. input_tokens=161, output_tokens=56
12:57:15,198 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:15,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.844000000000051. input_tokens=179, output_tokens=87
12:57:15,377 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:15,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.719000000000051. input_tokens=183, output_tokens=55
12:57:15,582 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:15,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.563000000000102. input_tokens=158, output_tokens=64
12:57:15,809 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:15,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.0. input_tokens=189, output_tokens=86
12:57:16,389 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:16,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.17200000000048. input_tokens=166, output_tokens=35
12:57:16,549 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:16,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.592999999999847. input_tokens=229, output_tokens=128
12:57:16,863 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:16,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.125. input_tokens=212, output_tokens=94
12:57:17,5 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:17,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.015999999999622. input_tokens=247, output_tokens=176
12:57:17,129 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:17,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.5789999999997235. input_tokens=190, output_tokens=115
12:57:17,298 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:17,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.311999999999898. input_tokens=164, output_tokens=62
12:57:17,689 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:17,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.171999999999571. input_tokens=168, output_tokens=86
12:57:17,844 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:17,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.858999999999469. input_tokens=239, output_tokens=166
12:57:18,121 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
12:57:18,122 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
12:57:18,131 datashaper.workflow.workflow INFO executing verb create_final_entities
12:57:18,172 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
12:57:18,423 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
12:57:18,424 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
12:57:18,434 datashaper.workflow.workflow INFO executing verb create_final_nodes
12:57:18,604 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
12:57:18,808 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
12:57:18,809 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
12:57:18,822 datashaper.workflow.workflow INFO executing verb create_final_communities
12:57:18,935 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
12:57:19,178 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
12:57:19,178 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
12:57:19,231 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
12:57:19,244 datashaper.workflow.workflow INFO executing verb create_final_relationships
12:57:19,286 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
12:57:19,491 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
12:57:19,491 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
12:57:19,492 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
12:57:19,498 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
12:57:19,515 datashaper.workflow.workflow INFO executing verb create_final_text_units
12:57:19,546 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
12:57:19,807 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships', 'create_final_communities']
12:57:19,809 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
12:57:19,816 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
12:57:19,821 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
12:57:19,838 datashaper.workflow.workflow INFO executing verb create_final_community_reports
12:57:19,853 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 97
12:57:19,928 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 227
12:57:32,830 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:32,832 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:32,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.75. input_tokens=2147, output_tokens=496
12:57:35,700 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:35,702 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:35,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.657000000000153. input_tokens=2390, output_tokens=549
12:57:35,922 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:35,924 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:35,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.905999999999949. input_tokens=2111, output_tokens=590
12:57:36,290 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:36,291 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:36,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.28099999999995. input_tokens=2026, output_tokens=525
12:57:36,961 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:36,963 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:36,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.875. input_tokens=2260, output_tokens=528
12:57:37,971 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:37,973 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:37,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.90599999999995. input_tokens=2091, output_tokens=546
12:57:39,763 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:39,765 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:39,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.71900000000005. input_tokens=2362, output_tokens=550
12:57:41,844 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:41,846 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:41,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.78099999999995. input_tokens=2786, output_tokens=752
12:57:44,709 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:44,711 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:44,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.67200000000048. input_tokens=2113, output_tokens=589
12:57:45,276 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:45,278 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:45,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.21900000000005. input_tokens=2353, output_tokens=650
12:57:47,263 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:47,266 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:47,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.23400000000038. input_tokens=2734, output_tokens=759
12:57:49,56 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:49,57 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:49,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.01599999999962. input_tokens=3163, output_tokens=921
12:57:49,800 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:49,801 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:49,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.765000000000327. input_tokens=2391, output_tokens=563
12:57:59,581 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:57:59,583 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:57:59,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.5630000000001. input_tokens=5891, output_tokens=778
12:58:04,597 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:04,601 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:04,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 44.51499999999942. input_tokens=2161, output_tokens=761
12:58:19,691 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:19,692 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:19,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.983999999999469. input_tokens=2053, output_tokens=529
12:58:20,300 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:20,302 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:20,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.561999999999898. input_tokens=2144, output_tokens=582
12:58:20,538 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:20,539 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:20,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.813000000000102. input_tokens=2057, output_tokens=520
12:58:23,880 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:23,881 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:23,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.21900000000005. input_tokens=2139, output_tokens=592
12:58:24,312 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:24,313 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:24,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.64099999999962. input_tokens=2187, output_tokens=549
12:58:25,563 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:25,564 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:25,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.811999999999898. input_tokens=2250, output_tokens=584
12:58:26,868 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:26,870 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:26,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.203999999999724. input_tokens=2404, output_tokens=636
12:58:26,998 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:26,999 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:27,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.25. input_tokens=2961, output_tokens=575
12:58:28,569 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:28,572 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:28,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.89099999999962. input_tokens=2192, output_tokens=588
12:58:29,291 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:29,293 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:29,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.60900000000038. input_tokens=2112, output_tokens=528
12:58:30,610 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:30,612 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:30,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.875. input_tokens=2615, output_tokens=830
12:58:31,109 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:31,111 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:31,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.45300000000043. input_tokens=7702, output_tokens=898
12:58:31,685 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:31,686 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:31,689 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'list'>:
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 388 (char 387)
12:58:31,698 graphrag.llm.openai.openai_chat_llm WARNING error parsing llm json, retrying
12:58:34,88 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:34,90 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:34,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.32799999999952. input_tokens=2723, output_tokens=848
12:58:36,636 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:36,637 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:36,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.936999999999898. input_tokens=3706, output_tokens=866
12:58:38,860 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:38,861 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:38,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.14100000000053. input_tokens=3196, output_tokens=862
12:58:44,319 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:44,322 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:44,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.65599999999995. input_tokens=2783, output_tokens=819
12:58:46,522 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:58:46,523 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:58:46,524 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'list'>:
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 6 (char 5)
12:59:07,51 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
12:59:07,52 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
12:59:07,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 62.35900000000038. input_tokens=2416, output_tokens=641
12:59:07,70 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
12:59:07,364 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
12:59:07,365 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
12:59:07,387 datashaper.workflow.workflow INFO executing verb create_final_documents
12:59:07,399 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
12:59:07,745 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_community_reports', 'create_final_text_units', 'create_final_documents', 'create_final_relationships', 'create_final_entities']
12:59:07,746 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
12:59:07,756 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
12:59:07,764 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
12:59:07,772 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
12:59:07,779 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
12:59:07,803 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
12:59:07,812 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
12:59:07,813 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
12:59:07,820 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
12:59:08,172 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-3: TPM=0, RPM=0
12:59:08,173 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-3: 25
12:59:08,206 graphrag.index.operations.embed_text.strategies.openai INFO embedding 32 inputs via 32 snippets using 2 batches. max_batch_size=16, max_tokens=8191
12:59:08,531 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 400 Bad Request"
12:59:08,535 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': ["# Fezziwig's Christmas Eve Celebration Community\n\nThis community is centered around Fezziwig's Christmas Eve party, an event that brings together various entities including Fezziwig himself, his family, employees, and attendees. The relationships between these entities highlight the significance of the event and the joyous atmosphere it creates.\n\n## Fezziwig as the community's central figure\n\nFezziwig is the central figure in this community, recognized for his love of dancing and his role in hosting the Christmas Eve party. His character is described as jolly and esteemed, which has a significant impact on the atmosphere of the event. [Data: Entities (80), Relationships (142, 143, 140, 141)]\n\n## The Christmas Eve party as a focal point\n\nThe Christmas Eve party is the pivotal event in this community, serving as the setting where all other entities interact. It is described as a memorable event filled with dancing and joy, suggesting a strong communal bond. [Data: Entities (90), Relationships (142, 152, 148, 151, 149, 150)]\n\n## Dancing as a significant activity\n\nDancing is a significant activity that took place during Fezziwig's Christmas Eve party, facilitated by the fiddler's music. This traditional form of celebration underscores the community's connection to old customs and the importance of shared experiences. [Data: Entities (97), Relationships (152, 147)]\n\n## Fezziwig's impact on his apprentices\n\nFezziwig's interaction with his apprentices highlights his positive influence on them. The apprentices express deep gratitude and admiration for Fezziwig, indicating that he is respected and appreciated for his guidance and the effect he has on those around him. [Data: Relationships (143)]\n\n## Family bonds within the community\n\nThe close relationship between Fezziwig and his family, including his wife and daughters, is evident in their joint participation in the Christmas Eve party. Their interactions during the event showcase a strong family bond and a shared sense of joy in the celebration. [Data: Relationships (140, 141)]\n\n## The role of music in the celebration\n\nThe fiddler's role in providing music for the dances at the party is integral to the celebration. The music not only facilitates the dancing but also contributes to the overall festive atmosphere, emphasizing the communal nature of the event. [Data: Entities (89), Relationships (146, 147)]\n\n## Community participation in the event\n\nThe Christmas Eve party is characterized by the participation of a diverse group of attendees, including Fezziwig's family, employees, and friends. This wide range of participants suggests a strong sense of community and the importance of the event in bringing people together. [Data: Relationships (148)]\n\n## The significance of dance couples\n\nThe presence of specific dance couples, such as the old top couple, new top couple, and the bottom couple, adds structure to the event's dancing. These couples may represent different generations or statuses within the community, highlighting the event's traditional and hierarchical aspects. [Data: Relationships (149, 150, 151)]", "# Scrooge and Christmas Carol Community\n\nThe community is centered around Scrooge's interactions with a Christmas carol and a discussion with a gentleman concerning the poor and the Christmas season. The entities involved are interconnected through the themes of the Christmas season and social responsibility.\n\n## Scrooge's reaction to the Christmas carol\n\nScrooge's reaction to the Christmas carol sung by the young nose owner is a central aspect of this community. His response could indicate his attitude towards the festive season and his willingness to engage with the community's expectations. This interaction is significant as it sets the stage for Scrooge's character development and potential change in perspective. [Data: Relationships (55)]\n\n## The gentleman's conversation with Scrooge\n\nThe gentleman's conversation with Scrooge about the poor and the upcoming Christmas season is a key event in this community. It highlights the societal expectations of charity and compassion during the festive season. This interaction is crucial in understanding the social dynamics and the moral lessons associated with Scrooge's character. [Data: Relationships (50)]\n\n## Association of the poor and destitute with the Christmas carol\n\nThe poor and destitute are associated with the Christmas carol being sung, suggesting a connection between the themes of the carol and the plight of the less fortunate. This linkage underscores the carol's role in bringing attention to social issues and the need for charitable actions, especially during the Christmas season. [Data: Relationships (121)]\n\n## The gentleman advocating for the poor and destitute\n\nThe gentleman's advocacy for the poor and destitute is an essential element within this community, reflecting the social consciousness of the time. His actions and words could inspire others to consider their responsibilities towards those in need, potentially leading to societal changes or shifts in public opinion. [Data: Relationships (120)]\n\n## The Christmas carol as a symbol of the season's spirit\n\nThe Christmas carol itself is a symbol of the season's spirit and joy, which contrasts with the depiction of the poor and destitute. This duality represents the community's complex view of the Christmas season, encompassing both celebration and the awareness of social issues. [Data: Entities (38)]", "# Cratchit Family Christmas Celebration\n\nThe community is centered around the Cratchit family's celebration of Christmas Day, which includes a significant dinner gathering. The entities involved are Christmas Day, the Cratchit Family, and the Christmas Dinner, with relationships highlighting the importance of the holiday to the family and their activities on this day.\n\n## Christmas Day as a pivotal event\n\nChristmas Day is a pivotal event in the story, symbolizing the spirit of the holiday and playing a key role in Scrooge's transformation. It is celebrated by the Cratchit family, which underscores the significance of this day in their lives. [Data: Entities (120), Relationships (102, 167)]\n\n## Cratchit Family as the core entity\n\nThe Cratchit Family is the core entity of this community, representing the family unit that celebrates Christmas Day together. Their celebration is an embodiment of the holiday's spirit and is central to the story's themes of unity and joy. [Data: Entities (122), Relationships (167, 168)]\n\n## Christmas Dinner as a central celebration\n\nThe Christmas Dinner is a central event in the Cratchit family's celebration of Christmas Day. It is an occasion where the family comes together to feast, which is an important tradition in many cultures and symbolizes the warmth and togetherness of family bonds. [Data: Entities (129), Relationships (168)]\n\n## Scrooge's transformation linked to Christmas Day\n\nScrooge's transformation, which is a central theme of the story, is closely linked to Christmas Day. His acts of kindness and change of heart occur on this day, illustrating the holiday's potential to inspire personal growth and redemption. [Data: Relationships (102)]\n\n## The importance of family bonds\n\nThe relationships between the Cratchit family members and their celebration of Christmas Day underscore the importance of family bonds. The story emphasizes the significance of these bonds and their role in creating a supportive and loving environment, especially during festive occasions. [Data: Entities (122), Relationships (167)]", "# Scrooge's Past and the Golden Idol\n\nThe community is centered around Scrooge's past, particularly his younger self and the golden idol, which symbolizes his pursuit of wealth. The relationships between Scrooge, the golden idol, and the fair young girl reveal the impact of Scrooge's avarice on his personal relationships and character transformation.\n\n## Scrooge's connection to the golden idol\n\nScrooge's pursuit of the golden idol represents his avarice and the transformation in his character. This relationship indicates the central theme of wealth pursuit in Scrooge's life, which has led to a loss of personal relationships. [Data: Entities (100), Relationships (85)]\n\n## Link between Scrooge's former self and the golden idol\n\nThe golden idol is not only connected to Scrooge's present self but also to his former self, suggesting that the pursuit of wealth has been a long-standing trait. This connection highlights the continuity of Scrooge's values and the impact they have had on his life. [Data: Relationships (85, 154)]\n\n## The fair young girl as a victim of Scrooge's wealth pursuit\n\nThe fair young girl, who was once in a relationship with Scrooge's former self, has been displaced by the golden idol. This displacement signifies the destructive impact of wealth pursuit on personal relationships and emotional connections. [Data: Entities (99), Relationships (153, 154)]\n\n## Scrooge's former self as a reflection of his past\n\nScrooge's connection to his former self, facilitated by the Ghost of Christmas Past, is a crucial element in understanding his character development. This relationship serves as a reminder of the innocence and potential that Scrooge once had before his transformation into avarice. [Data: Entities (101), Relationships (84)]\n\n## The golden idol as a symbol of societal values\n\nThe golden idol's presence in Scrooge's past and its impact on his life suggest that it symbolizes more than just personal wealth. It represents the societal values that prioritize material wealth over personal connections and emotional well-being. [Data: Entities (100), Relationships (85, 154)]", "# The Scrooge Network: Interactions with Old Joe and Associates\n\nThis community is centered around the character Scrooge and his interactions with Old Joe and other individuals involved in various transactions and acquaintances. The entities within this community are connected through their relationships and activities, primarily dealing with stolen goods and other transactions.\n\n## Old Joe as a central figure in transactions\n\nOld Joe is a pivotal character in this community, known for dealing in stolen goods and interacting with various individuals such as Scrooge, Mrs. Dilber, the laundress, and the undertaker's man. His transactions and relationships suggest a network of illegal activities. [Data: Entities (171), Relationships (98, 189, 188, 190)]\n\n## Scrooge's encounter with the criminal network\n\nScrooge, accompanied by the Phantom, encounters Old Joe and witnesses the appraisal of stolen items. This encounter indicates Scrooge's indirect involvement with the criminal activities within this community. [Data: Relationships (98)]\n\n## Mrs. Dilber's role in the stolen goods transactions\n\nMrs. Dilber regularly brings items to Old Joe for appraisal and sale, suggesting her role in the transactions of stolen goods. Her interactions with Old Joe and other characters indicate a network of individuals involved in these activities. [Data: Entities (172), Relationships (188, 191, 192)]\n\n## The laundress and her connections within the community\n\nThe laundress is familiar with Old Joe and has connections with Mrs. Dilber and the undertaker's man. This suggests her possible involvement in the same circle of transactions and illegal activities. [Data: Entities (173), Relationships (189, 193, 191)]\n\n## The undertaker's man and his relationships\n\nThe undertaker's man recognizes Old Joe and has interactions with both the laundress and Mrs. Dilber. His connections within this community hint at a broader network of individuals possibly involved in related activities. [Data: Entities (174), Relationships (190, 193, 192)]\n\n## Nature of transactions and potential criminal activities\n\nThe transactions and relationships among these characters suggest a network potentially involved in criminal activities, such as the trade of stolen goods. The interactions and familiarity among these individuals point to a structured operation. [Data: Relationships (98, 189, 188, 190, 193, 191, 192)]", "# Maritime Christmas Community\n\nThis community is centered around a ship (SHIP) and its crew during a Christmas period, with a focus on the roles of the look-out (LOOK-OUT), officers (OFFICERS), and ship's helmsman (SHIP'S HELMSMAN), as observed through Scrooge's vision in the presence of the Ghost.\n\n## SHIP as the central entity of the community\n\nSHIP is the central entity in this maritime community, acting as the base where all other roles and activities are situated. The ship's festive mood indicates a possible celebration, which is a unique aspect of this community. [Data: Entities (146), Relationships (91, 181, 182, 180)]\n\n## LOOK-OUT's position and responsibility\n\nThe LOOK-OUT is an essential role on the ship, positioned at the bow to watch for any potential obstacles or dangers. This role is critical for the safety of the ship and its crew. [Data: Entities (153), Relationships (181)]\n\n## OFFICERS managing the watch on the ship\n\nOFFICERS are crew members who have the responsibility of managing the watch on the ship. Their role is crucial for the ship's operation and the crew's well-being during the watch. [Data: Entities (154), Relationships (182)]\n\n## SHIP'S HELMSMAN steering the ship\n\nSHIP'S HELMSMAN is the person responsible for steering the ship as observed in the Ghost's vision. This role is vital for navigating the ship safely through the waters. [Data: Entities (152), Relationships (180)]\n\n## Scrooge's vision and the ship's festive spirit\n\nScrooge's vision of the ship and its crew in a Christmas mood suggests a contrast to his own experience. This observation is significant as it sets the context for the maritime community's atmosphere and the period's festivities. [Data: Relationships (91)]", "# Scrooge and Marley Business Community\n\nThe community is centered around the business partnership of Scrooge and Marley, involving the spectral presence of the deceased Marley, the ongoing operations of their firm, and its interactions with external entities such as charity collectors and the warehouse that displays their sign.\n\n## Continuation of Scrooge and Marley partnership posthumously\n\nThe business partnership of Scrooge and Marley continues to operate under the same name even after Marley's death. This indicates the legacy and brand value associated with the partnership, which may have enduring implications on the business community. [Data: Entities (15, 14, 39); Relationships (41, 42)]\n\n## Marley's spectral connection to Scrooge\n\nMarley's ghostly appearance to Scrooge signifies a profound supernatural connection between the two. This haunting encounter is central to the narrative and underscores the past business relationship and the unresolved issues that Scrooge must confront. [Data: Entities (14); Relationships (40)]\n\n## The warehouse as a physical representation of the firm\n\nThe warehouse where the Scrooge and Marley sign is displayed serves as a tangible symbol of the firm's presence and operations. This location may play a role in the business's ongoing activities and legacy. [Data: Entities (16); Relationships (42)]\n\n## Charity collectors' visit to Scrooge's office\n\nThe visit of the two portly gentlemen to Scrooge's office in relation to the firm's activities suggests the firm's engagement with charitable endeavors or the community's perception of the firm's social responsibility. [Data: Entities (30); Relationships (43)]\n\n## The spectral visitation as a catalyst for change\n\nMarley's ghostly visitation to Scrooge can be interpreted as a catalyst for Scrooge's moral and personal transformation. This supernatural event is a pivotal moment in the story and may symbolize the potential for significant change within the community or in Scrooge's character. [Data: Entities (14); Relationships (40)]\n\n## The business partnership's historical significance\n\nThe historical significance of the Scrooge and Marley partnership is underlined by the continued use of the name and the spectral visitation. This legacy may influence the business practices and reputation of the firm within the community. [Data: Entities (15, 39); Relationships (41)]", "# Scrooge Family Dynamics\n\nThe community is centered around the family dynamics involving Master Scrooge and his younger sister. The relationships suggest a temporal and familial connection, with significant implications for understanding the background and development of the character Master Scrooge.\n\n## Master Scrooge as a younger version of Scrooge\n\nThe entity 'Master Scrooge' represents a younger version of the character Scrooge, as evidenced by the description and the temporal relationship with the 'Scrooge' entity. This suggests a look into the past of the character, providing insight into his earlier life and development. [Data: Entities (75), Relationships (76)]\n\n## Sister's role in Master Scrooge's life\n\nThe 'Sister' entity is introduced as the younger sister of Master Scrooge, coming to bring him home for the holidays. This indicates a close familial bond and suggests a significant influence on Master Scrooge's life during his younger years. [Data: Entities (76), Relationships (139)]\n\n## Temporal relationship between Master Scrooge and Scrooge\n\nThe relationship between 'Master Scrooge' and 'Scrooge' signifies a temporal connection, suggesting that the story of Master Scrooge is a prequel to the story of Scrooge. This is important for understanding the character's evolution and the context of his future actions. [Data: Relationships (76)]\n\n## Familial bond as a key theme\n\nThe familial bond between Master Scrooge and his sister is a central theme in this community of entities. It underscores the importance of family in shaping the character's values and experiences. [Data: Relationships (139)]\n\n## Significance of the holiday context\n\nThe mention of the holidays in the relationship description indicates that the setting is significant for the narrative. It may provide a backdrop for key moments of character development and interaction. [Data: Relationships (139)]", "# The Cratchit Family and Scrooge's Benevolence\n\nThe community is centered around the Cratchit family, with key relationships to Ebenezer Scrooge and the events of Christmas. The entities' interactions and Scrooge's transformation play a significant role in the dynamics of this community, highlighting themes of empathy and social concern.\n\n## Bob Cratchit as the central figure of the Cratchit family\n\nBob Cratchit is the head of the Cratchit family, known for his cheerful demeanor and dedication to his job and family. His relationship with Scrooge, his employer, is central to the story, as it demonstrates the contrast between their economic statuses and the potential for kindness in the workplace. Bob's role as a father to Tiny Tim and his interactions with his other children and wife Mrs. Cratchit underscore the family's close-knit dynamics. [Data: Entities (5), Relationships (8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)]\n\n## Tiny Tim's condition and its impact on the family\n\nTiny Tim, the sickly son of Bob Cratchit, is a focal point of the family's concerns and is significant to the community's narrative. His condition, noted as a cripple, is a symbol of the challenges the Cratchit family faces. Despite his illness, Tiny Tim's positive outlook and the family's care for him reflect the community's resilience and hope. [Data: Entities (118), Relationships (165, 166, 128)]\n\n## Scrooge's transformation and its effects on the Cratchit family\n\nScrooge's transformation from a harsh employer to a benevolent figure is pivotal to the community's dynamics. His gift of a turkey to the Cratchit family and his subsequent concern for Tiny Tim indicate a significant shift in his character. This change has a direct impact on the Cratchit family, improving their circumstances and highlighting the potential for personal growth to influence community well-being. [Data: Relationships (9, 88)]\n\n## The Cratchit family's Christmas celebration as a symbol of unity\n\nThe Cratchit family's Christmas celebration is a symbol of unity and joy despite their modest means. The family's preparation for the meal and their togetherness during the celebration are indicative of their strong bonds. This event is significant within the community as it showcases the importance of family and community support. [Data: Relationships (10, 12, 13, 14, 15, 16, 17, 18, 19)]\n\n## The role of Scrooge's nephew in offering support\n\nScrooge's nephew plays a role in the community by showing concern for Bob Cratchit and offering his help. This act of compassion suggests a sense of community and the willingness of individuals to support each other, contributing to the overall well-being of the community. [Data: Relationships (18)]\n\n## The social interactions of the Cratchit children\n\nThe interactions of the Cratchit children, including Martha, Belinda, Peter, and Master Peter, reflect the family's dynamics and their integration into the community. Their assistance in meal preparation and their relationships with their parents and each other provide insight into the family's structure and the roles each member plays. [Data: Relationships (17, 12, 13, 14, 16)]", "# Bob Cratchit's Christmas Celebration\n\nThe community is centered around Bob Cratchit's dwelling and the Christmas Present entity, highlighting a spiritual connection and the location of a significant Christmas celebration. The entities and their relationships reflect the importance of the event and the atmosphere of the celebration.\n\n## Bob Cratchit's dwelling as the heart of the celebration\n\nBob Cratchit's dwelling is the physical location where the Christmas celebration takes place, indicating its central role in the community. The dwelling is blessed by Christmas Present, which signifies its importance in the narrative and the celebration. [Data: Entities (123), Relationships (15, 169)]\n\n## Christmas Present's spiritual connection to the event\n\nChristmas Present smiles and blesses Bob Cratchit's dwelling, which indicates a profound spiritual connection to the event. This relationship underscores the significance of the celebration and the entities involved. [Data: Relationships (15, 169)]\n\n## The significance of the Christmas celebration\n\nThe Christmas celebration at Bob Cratchit's dwelling is a key event in the community, representing joy and unity. The presence of Christmas Present and the blessing of the dwelling emphasize the importance of this event in the community's structure. [Data: Entities (127), Relationships (15, 169)]\n\n## Bob Cratchit's role in the community\n\nBob Cratchit is implicitly a central figure in the community, as his dwelling is the location of the Christmas celebration. The relationship between Bob Cratchit and Christmas Present suggests a pivotal role in the event and the community's dynamics. [Data: Relationships (15)]\n\n## The community's focus on traditional values\n\nThe entities and their relationships in this community reflect a focus on traditional values and the importance of community and family during the Christmas season. This traditional aspect minimizes potential conflict or negative impact within the community. [Data: Entities (123, 127), Relationships (15, 169)]", "# The Christmas Spirits and Ebenezer Scrooge\n\nThis community is centered around Ebenezer Scrooge and his transformative journey guided by the three ghosts of Christmas. The entities include the spirits of Christmas Past, Present, and Yet to Come, Scrooge himself, and various characters from his life. The relationships and interactions among these entities reveal the narrative of Scrooge's redemption.\n\n## The central role of Ebenezer Scrooge\n\nEbenezer Scrooge is the central figure in this community, with relationships to all three ghosts and several other characters. His transformation from a miser to a more generous and empathetic person is the focal point of the story. Scrooge's interactions with the ghosts and other characters highlight his journey and the changes he undergoes [Data: Entities (13); Relationships (30, 28, 34, 8, 36, 22, 29, 27, 33, 38, 37, 20, 21, 35)].\n\n## The ghosts' influence on Scrooge\n\nThe ghosts of Christmas Past, Present, and Yet to Come play a pivotal role in Scrooge's transformation. Each spirit provides Scrooge with insights into his past, present, and future, guiding him towards a change in behavior and attitude. Their visits are powerful and transformative moments in the story [Data: Entities (10, 9, 11); Relationships (30, 28, 34)].\n\n## Scrooge's strained relationships\n\nScrooge's relationships with his nephew Fred and his employee Bob Cratchit are strained. These relationships highlight Scrooge's initial lack of empathy and his transformation towards the end of the story. The interactions with these characters serve as a stark contrast to the warmth and generosity that the ghosts represent [Data: Relationships (36, 22, 8, 20)].\n\n## The significance of the Christmas setting\n\nChristmas serves as the backdrop for Scrooge's transformation. The holiday's associated festivities, such as the march of the ghosts and the preparations by the bakers, underscore the contrast between Scrooge's coldness and the warmth of the holiday spirit. The setting is integral to the narrative and Scrooge's change of heart [Data: Relationships (31, 32)].\n\n## The role of symbols in the community\n\nSymbols such as the candles and fog play a metaphorical role in the story, representing the contrast between light and darkness, warmth and coldness, and clarity and obscurity. These symbols reflect Scrooge's character and his journey towards enlightenment [Data: Relationships (38, 37)].\n\n## The transformative power of the supernatural\n\nThe supernatural elements of the story, specifically the ghosts, are instrumental in Scrooge's transformation. Their ability to transcend time and reveal the consequences of actions underscore the transformative power of the supernatural in this community [Data: Entities (10, 9, 11); Relationships (30, 28, 34)].\n\n## The impact of Scrooge's transformation\n\nScrooge's transformation has a significant impact on his community. His change in behavior influences his relationships with family, employees, and others, potentially leading to a more joyful and empathetic community atmosphere. The story emphasizes the ripple effect of individual change on the broader community [Data: Relationships (8, 20, 36, 22)].", "# Scrooge's Market-Town Memories\n\nThe community is centered around Scrooge's memories of a market-town, which includes specific landmarks such as a bridge, a church, and a river. The entities and their relationships reveal a nostalgic connection to Scrooge's past.\n\n## Scrooge's personal connection to the market-town\n\nScrooge has a strong personal connection to the market-town from his youth, remembering every detail with clarity. This suggests that the market-town holds significant emotional value to Scrooge. [Data: Relationships (67)]\n\n## The market-town's landmarks and their significance\n\nThe market-town contains specific landmarks that are part of Scrooge's past memories, including a bridge, a church, and a river. These landmarks contribute to the vividness of Scrooge's memories and may represent important aspects of his past experiences. [Data: Relationships (135, 136, 137)]\n\n## The bridge as a memorable feature\n\nThe bridge within the market-town is a memorable feature from Scrooge's past. Its inclusion in Scrooge's memories indicates that it may have been a significant or frequently visited spot. [Data: Relationships (135)]\n\n## The church's role in Scrooge's history\n\nThe church is another landmark that plays a role in Scrooge's historical memories. It may represent a place of sanctuary or community in Scrooge's past. [Data: Relationships (136)]\n\n## The river's presence in Scrooge's memories\n\nThe presence of a winding river in Scrooge's memories suggests that natural elements and topography were influential in shaping his experiences in the market-town. [Data: Relationships (137)]\n\n## The market-town's overall impact on Scrooge's development\n\nThe market-town's various elements, including the bridge, church, and river, collectively contributed to Scrooge's personal development and shaped his perception of the world. The degree of detail Scrooge remembers indicates a profound impact on his life. [Data: Entities (61), Relationships (67, 135, 136, 137)]", "# The Gathered Family and the Man's Death\n\nThis community is centered around a family gathering during Christmas, where the mood is affected by the news of a man's death. The entities within this community are primarily characterized by their relationships to the event and to each other, with the man's death serving as a pivotal point that influences the atmosphere and interactions among the characters.\n\n## The children as a central focus of the gathering\n\nThe children are a lively group that is central to the family gathering. Their actions and interactions with other characters are pivotal to the scene. Their happiness and excitement are, however, marred by the news of the man's death [Data: Entities (105), Relationships (90, 157, 158, 159)].\n\n## The emotional impact of the man's death on the gathering\n\nThe man's death has a significant emotional impact on the gathering, influencing the mood and interactions of the family members. This event is a central theme that weaves through the relationships and dynamics of the community [Data: Entities (180), Relationships (161, 196, 197)].\n\n## Scrooge's reflection on the man's death and its implications\n\nScrooge is seen reflecting on the man's death and its implications, suggesting a deeper connection or a moral lesson that the story aims to convey. This reflection adds a layer of complexity to the community's dynamics [Data: Relationships (99)].\n\n## The family gathering as a Christmas event\n\nThe family gathering is a Christmas event, which adds a layer of traditional significance to the community. The celebration is disrupted by the news of the man's death, creating a contrast between joy and sorrow [Data: Relationships (112)].\n\n## The role of the comely matron and her daughter in the community\n\nThe comely matron and her daughter play a role in the interactions with the children, suggesting a generational bond and continuity within the family. The comely matron is also a representation of the young girl from Scrooge's past, highlighting the theme of growth and change within the community [Data: Entities (102, 103, 104), Relationships (155, 156)].\n\n## The husband's role in delivering the news of the man's death\n\nThe husband's role is significant as he is the one who brings the news of the man's death to the mother, thus directly impacting the emotional state of the family gathering. His actions serve as a catalyst for the emotional reactions of the other characters [Data: Entities (182), Relationships (198)].\n\n## The mother's anxious wait and her reaction to the news\n\nThe mother is depicted as anxiously waiting for her husband, and her reaction to the news of the man's death is a pivotal moment in the community's dynamics. Her emotional state is emblematic of the impact that the news has on the family gathering [Data: Entities (181), Relationships (198)].\n\n## The emotional impact of the man's death on the town\n\nThe man's death is not only significant to the family but also has an emotional impact on the broader community of the town. This suggests that the event has far-reaching consequences beyond the immediate family circle [Data: Entities (183), Relationships (197)].", "# Scrooge's Counting-House Climate\n\nThe community is centered around the working conditions within Scrooge's counting-house, with the CLERK being significantly affected by Scrooge's miserliness. The entities and their relationships reveal a situation where the lack of warmth and holiday consideration could impact employee morale and well-being.\n\n## Scrooge's direct influence on the working environment\n\nScrooge's control over the coal-box and the clerk's working conditions is a central issue in this community. The clerk's cold working environment is a direct result of Scrooge's miserliness, which affects the employee's ability to stay warm. This condition is indicative of Scrooge's approach to managing his counting-house. [Data: Relationships (47, 116)]\n\n## The clerk's struggle with cold working conditions\n\nThe clerk is forced to work in a small, dimly lit cell within Scrooge's counting-house, which is poorly heated. The use of a white comforter by the clerk signifies a desperate attempt to keep warm, highlighting the inadequate conditions of the workplace. [Data: Entities (22, 28); Relationships (114, 117)]\n\n## Discussion of Christmas Day off\n\nThe clerk's desire to take Christmas Day off is an aspect of the community that reflects on the employee's need for a holiday break. This discussion indicates the clerk's dissatisfaction with the current working conditions and the lack of holiday consideration. [Data: Relationships (109)]\n\n## The counting-house as the core of the community\n\nThe counting-house is the central location where the entities interact, and it sets the tone for the working conditions within the community. The environment of the counting-house, influenced by Scrooge's attitude, shapes the experiences of the clerk and is central to understanding the dynamics of this community. [Data: Relationships (114)]\n\n## The coal-box as a symbol of Scrooge's control\n\nThe coal-box, which the clerk cannot access, symbolizes Scrooge's control and miserliness. It is a physical representation of the power dynamics within the community, where Scrooge's actions directly impact the clerk's quality of life at work. [Data: Entities (27); Relationships (116)]", "# Scrooge's Christmas Community\n\nThe community is centered around Scrooge and his family, particularly his nephew Fred and niece, with the Christmas celebration at Fred's house as the pivotal event. The relationships between family members and their interactions during the Christmas party provide insights into the dynamics of this community.\n\n## Fred as the central figure organizing the Christmas celebration\n\nFred, Scrooge's nephew, is the central figure in organizing the Christmas celebration. He extends an invitation to Scrooge and is connected to other characters through social bonds. Fred's role in the community is significant as he bridges the gap between Scrooge and the rest of the family. [Data: Entities (8), Relationships (23, 24, 25, 26)]\n\n## Strained relationship between Scrooge and his family\n\nScrooge maintains a strained relationship with both his nephew and niece. Despite the tension, the family dynamics are crucial to understanding the community's structure, as the Christmas celebration brings them together, showcasing the complexity of their relationships. [Data: Relationships (22, 48, 92)]\n\n## The Christmas celebration as a unifying event\n\nThe Christmas celebration at Fred's house is a unifying event that connects all the characters in the community. It serves as a backdrop for various interactions and relationships, highlighting the importance of tradition and family in this community. [Data: Relationships (24)]\n\n## Social interactions and bonds within the community\n\nThe community is characterized by social interactions and bonds, such as those between Fred and Topper, the plump sister, and Scrooge's niece's sisters. These interactions indicate a close-knit group of characters that engage in festive activities and conversations. [Data: Relationships (25, 26, 118, 119, 185, 184)]\n\n## Scrooge's niece and her influence on the community\n\nScrooge's niece is an influential figure within the community due to her strained relationship with Scrooge and her musical talent. Her presence adds a layer of complexity to the family dynamics and the Christmas celebration. [Data: Entities (155), Relationships (92, 183)]", "# The Christmas Spirits and Scrooge's Redemption\n\nThis community is centered around the interactions of Scrooge with supernatural entities, primarily the GHOST and JACOB MARLEY, who guide Scrooge through visions to learn the error of his ways. The entities associated with this community play significant roles in a Christmas-themed narrative intended to lead Scrooge to redemption.\n\n## Scrooge's direct interaction with the GHOST\n\nScrooge, the main character in this narrative, is directly haunted by the GHOST, which is the manifestation of the spirit of Jacob Marley. This interaction is central to the community's structure, as it sets the stage for Scrooge's transformation. The GHOST serves as a guide, leading Scrooge through a series of visions to help him understand the consequences of his actions [Data: Relationships (61)]\n\n## Jacob Marley's spectral intervention\n\nJacob Marley, a deceased business partner of Scrooge, appears as a ghost to Scrooge, serving as a harbinger of the moral lessons Scrooge is about to learn. Marley's spectral form acts as a conduit for Scrooge's redemption, highlighting the profound connection between the two spirits and their shared past [Data: Relationships (62, 127); Entities (53)]\n\n## The Christmas association with the supernatural entities\n\nThe ghosts visiting Scrooge are associated with Christmas, which is a significant aspect of this community. The timing and context of the supernatural interventions are linked to the holiday season, adding a layer of thematic importance to the narrative [Data: Relationships (110)]\n\n## The GHOST's role in Scrooge's moral transformation\n\nThe GHOST plays a pivotal role in Scrooge's moral transformation by providing a vision of Tiny Tim's potential fate. This vision is a turning point for Scrooge, compelling him to change his ways and become a more compassionate individual [Data: Relationships (128)]\n\n## Scrooge's past and future revealed through visions\n\nAs part of Scrooge's redemption, the GHOST reveals scenes from Scrooge's past and potential future. These visions are crucial in illustrating the impact of Scrooge's actions and serve as a catalyst for his transformation [Data: Entities (54)]\n\n## The supernatural journey includes physical locations\n\nDuring Scrooge's supernatural journey, he and the GHOST visit physical locations such as a lighthouse and a moor. These visits are symbolic and serve to enhance the narrative's themes of guidance and reflection [Data: Relationships (130, 129)]\n\n## The counting-house as a backdrop to Marley's connection\n\nThe counting-house is mentioned in relation to Jacob Marley, indicating a shared professional history with Scrooge. This detail underscores the depth of Marley's connection to Scrooge and the relevance of their past business relationship to the narrative [Data: Relationships (115)]\n\n## Holiday cheer contrasted with the supernatural elements\n\nIn contrast to the eerie supernatural elements, there is a moment of holiday cheer depicted at the lighthouse with two men sharing a Merry Christmas wish. This contrast serves to highlight the juxtaposition between the darkness of Scrooge's past and the potential for redemption and joy in his future [Data: Relationships (179); Entities (144, 145)]"]}
12:59:08,537 datashaper.workflow.workflow ERROR Error executing verb "generate_text_embeddings" in generate_text_embeddings: Error code: 400 - {'error': {'code': '1210', 'message': 'API '}}
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\workflows\v1\subflows\generate_text_embeddings.py", line 56, in generate_text_embeddings
    await generate_text_embeddings_flow(
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\flows\generate_text_embeddings.py", line 106, in generate_text_embeddings
    await _run_and_snapshot_embeddings(
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\flows\generate_text_embeddings.py", line 129, in _run_and_snapshot_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 92, in embed_text
    return await _text_embed_with_vector_store(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 182, in _text_embed_with_vector_store
    result = await strategy_exec(
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 62, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 106, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 100, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\resources\embeddings.py", line 236, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\_base_client.py", line 1843, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\_base_client.py", line 1537, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\_base_client.py", line 1638, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': '1210', 'message': 'API '}}
12:59:08,546 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "generate_text_embeddings" in generate_text_embeddings: Error code: 400 - {'error': {'code': '1210', 'message': 'API '}} details=None
12:59:08,555 graphrag.index.run.run ERROR error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\run\run.py", line 269, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\run\workflow.py", line 105, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\datashaper\workflow\workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\workflows\v1\subflows\generate_text_embeddings.py", line 56, in generate_text_embeddings
    await generate_text_embeddings_flow(
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\flows\generate_text_embeddings.py", line 106, in generate_text_embeddings
    await _run_and_snapshot_embeddings(
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\flows\generate_text_embeddings.py", line 129, in _run_and_snapshot_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 92, in embed_text
    return await _text_embed_with_vector_store(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 182, in _text_embed_with_vector_store
    result = await strategy_exec(
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 62, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 106, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 100, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\resources\embeddings.py", line 236, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\_base_client.py", line 1843, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\_base_client.py", line 1537, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\openai\_base_client.py", line 1638, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': '1210', 'message': 'API '}}
12:59:08,561 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
12:59:08,592 graphrag.cli.index ERROR Errors occurred during the pipeline run, see logs for more details.
13:02:52,210 graphrag.cli.index INFO Logging enabled at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\logs\indexing-engine.log
13:02:52,214 graphrag.cli.index INFO Starting pipeline run for: 20241210-130252, dry_run=False
13:02:52,216 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "glm-4-air",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://open.bigmodel.cn/api/paas/v4",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": null,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "embedding-3",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
13:02:52,224 graphrag.index.create_pipeline_config INFO skipping workflows 
13:02:52,225 graphrag.index.run.run INFO Running pipeline
13:02:52,225 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\output
13:02:52,225 graphrag.index.input.load_input INFO loading input from root_dir=input
13:02:52,226 graphrag.index.input.load_input INFO using file storage for input
13:02:52,228 graphrag.index.storage.file_pipeline_storage INFO search D:\GISERR\graduate_study\pytorch\graphrag\ragtest\input for files matching .*\.txt$
13:02:52,229 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
13:02:52,233 graphrag.index.input.text INFO Found 1 files, loading 1
13:02:52,235 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
13:02:52,235 graphrag.index.run.run INFO Final # of rows loaded: 1
13:02:52,415 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
13:02:52,422 datashaper.workflow.workflow INFO executing verb create_base_text_units
13:02:53,375 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
13:02:53,376 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:02:53,384 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
13:02:53,393 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
13:02:53,733 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4-air: TPM=0, RPM=0
13:02:53,733 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4-air: 25
13:02:54,267 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
13:02:54,268 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:02:54,277 datashaper.workflow.workflow INFO executing verb create_final_entities
13:02:54,320 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
13:02:54,538 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
13:02:54,540 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:02:54,550 datashaper.workflow.workflow INFO executing verb create_final_nodes
13:02:54,720 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
13:02:54,923 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
13:02:54,924 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:02:54,936 datashaper.workflow.workflow INFO executing verb create_final_communities
13:02:55,41 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
13:02:55,332 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
13:02:55,333 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:02:55,334 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
13:02:55,365 datashaper.workflow.workflow INFO executing verb create_final_relationships
13:02:55,412 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
13:02:55,623 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
13:02:55,623 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:02:55,630 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:02:55,630 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:02:55,649 datashaper.workflow.workflow INFO executing verb create_final_text_units
13:02:55,677 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
13:02:55,928 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes', 'create_final_communities']
13:02:55,929 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:02:55,935 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
13:02:55,941 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
13:02:55,960 datashaper.workflow.workflow INFO executing verb create_final_community_reports
13:02:55,975 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 97
13:02:56,44 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 227
13:02:56,130 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,138 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,138 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,138 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,139 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,139 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,139 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,140 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,140 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,140 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,141 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,141 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,141 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,142 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,142 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,161 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,162 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,162 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,162 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,163 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,163 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,164 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,164 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,164 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,165 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,165 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,165 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,165 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,166 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,166 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,166 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,166 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:02:56,172 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
13:02:56,421 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
13:02:56,422 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
13:02:56,444 datashaper.workflow.workflow INFO executing verb create_final_documents
13:02:56,455 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
13:02:56,664 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_relationships', 'create_final_entities', 'create_final_documents', 'create_final_text_units', 'create_final_community_reports']
13:02:56,664 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:02:56,671 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:02:56,676 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
13:02:56,682 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
13:02:56,688 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
13:02:56,712 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
13:02:56,716 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
13:02:56,716 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
13:02:56,725 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
13:02:56,995 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-3: TPM=0, RPM=0
13:02:56,995 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-3: 25
13:02:57,19 graphrag.index.operations.embed_text.strategies.openai INFO embedding 32 inputs via 32 snippets using 2 batches. max_batch_size=16, max_tokens=8191
13:02:57,589 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:57,619 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:57,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9369999999998981. input_tokens=8098, output_tokens=0
13:02:58,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.155999999999949. input_tokens=8068, output_tokens=0
13:02:58,540 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
13:02:58,556 graphrag.index.operations.embed_text.strategies.openai INFO embedding 227 inputs via 227 snippets using 15 batches. max_batch_size=16, max_tokens=8191
13:02:58,781 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:58,807 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:58,867 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:58,908 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5630000000001019. input_tokens=1067, output_tokens=0
13:02:59,142 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,143 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,143 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,145 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,145 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,146 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,146 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,147 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,148 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6089999999994689. input_tokens=42, output_tokens=0
13:02:59,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8590000000003783. input_tokens=987, output_tokens=0
13:02:59,462 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,462 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:02:59,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1090000000003783. input_tokens=343, output_tokens=0
13:02:59,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.344000000000051. input_tokens=260, output_tokens=0
13:03:00,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5469999999995707. input_tokens=512, output_tokens=0
13:03:00,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.780999999999949. input_tokens=410, output_tokens=0
13:03:00,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.030999999999949. input_tokens=381, output_tokens=0
13:03:00,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.25. input_tokens=454, output_tokens=0
13:03:01,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.4840000000003783. input_tokens=665, output_tokens=0
13:03:01,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.70299999999952. input_tokens=340, output_tokens=0
13:03:01,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.9219999999995707. input_tokens=438, output_tokens=0
13:03:01,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.188000000000102. input_tokens=637, output_tokens=0
13:03:01,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.438000000000102. input_tokens=303, output_tokens=0
13:03:02,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.655999999999949. input_tokens=465, output_tokens=0
13:03:02,330 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
13:03:02,405 graphrag.index.operations.embed_text.strategies.openai INFO embedding 42 inputs via 42 snippets using 7 batches. max_batch_size=16, max_tokens=8191
13:03:02,897 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:03:02,936 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:03:02,949 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:03:02,953 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:03:03,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6720000000004802. input_tokens=7200, output_tokens=0
13:03:03,124 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:03:03,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7820000000001528. input_tokens=7200, output_tokens=0
13:03:03,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.875. input_tokens=7200, output_tokens=0
13:03:03,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9219999999995707. input_tokens=7055, output_tokens=0
13:03:03,388 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:03:03,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.030999999999949. input_tokens=7200, output_tokens=0
13:03:03,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=7200, output_tokens=0
13:03:08,87 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:03:08,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.828000000000429. input_tokens=7200, output_tokens=0
13:03:08,327 graphrag.cli.index INFO All workflows completed successfully.
13:13:53,195 graphrag.cli.index INFO Logging enabled at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\logs\indexing-engine.log
13:13:53,200 graphrag.cli.index INFO Starting pipeline run for: 20241210-131353, dry_run=False
13:13:53,201 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "glm-4-air",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://open.bigmodel.cn/api/paas/v4",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": null,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "embedding-3",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
13:13:53,212 graphrag.index.create_pipeline_config INFO skipping workflows 
13:13:53,212 graphrag.index.run.run INFO Running pipeline
13:13:53,213 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\output
13:13:53,215 graphrag.index.input.load_input INFO loading input from root_dir=input
13:13:53,215 graphrag.index.input.load_input INFO using file storage for input
13:13:53,218 graphrag.index.storage.file_pipeline_storage INFO search D:\GISERR\graduate_study\pytorch\graphrag\ragtest\input for files matching .*\.txt$
13:13:53,218 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
13:13:53,223 graphrag.index.input.text INFO Found 1 files, loading 1
13:13:53,227 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
13:13:53,228 graphrag.index.run.run INFO Final # of rows loaded: 1
13:13:53,466 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
13:13:53,472 datashaper.workflow.workflow INFO executing verb create_base_text_units
13:13:54,522 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
13:13:54,522 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:13:54,532 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
13:13:54,544 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
13:13:54,898 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4-air: TPM=0, RPM=0
13:13:54,898 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4-air: 25
13:14:01,476 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:01,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.530999999999949. input_tokens=2936, output_tokens=77
13:14:07,949 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:07,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.92200000000048. input_tokens=2936, output_tokens=408
13:14:08,560 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:08,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.594000000000051. input_tokens=2936, output_tokens=696
13:14:12,923 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:12,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.436999999999898. input_tokens=2937, output_tokens=633
13:14:13,790 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:13,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.82799999999952. input_tokens=2936, output_tokens=784
13:14:14,605 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:14,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.64100000000053. input_tokens=2937, output_tokens=592
13:14:16,291 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:16,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.28099999999995. input_tokens=2937, output_tokens=1074
13:14:16,456 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:16,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.45300000000043. input_tokens=2938, output_tokens=410
13:14:16,472 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:16,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.48399999999947. input_tokens=2937, output_tokens=898
13:14:17,42 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:17,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.03099999999995. input_tokens=2937, output_tokens=894
13:14:17,350 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:17,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.40599999999995. input_tokens=2937, output_tokens=577
13:14:17,673 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:17,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.640000000000327. input_tokens=2936, output_tokens=839
13:14:17,832 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:17,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.875. input_tokens=2937, output_tokens=664
13:14:19,541 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:19,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.592999999999847. input_tokens=2936, output_tokens=450
13:14:19,570 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:19,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.59400000000005. input_tokens=2937, output_tokens=1104
13:14:20,25 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:20,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.07800000000043. input_tokens=2936, output_tokens=1003
13:14:20,776 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:20,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.859999999999673. input_tokens=2936, output_tokens=738
13:14:22,400 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:22,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.39099999999962. input_tokens=2936, output_tokens=791
13:14:24,877 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:24,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.92199999999957. input_tokens=2935, output_tokens=792
13:14:24,936 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:24,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.936999999999898. input_tokens=2936, output_tokens=760
13:14:25,685 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:25,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.70299999999952. input_tokens=2936, output_tokens=944
13:14:26,862 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:26,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.875. input_tokens=2937, output_tokens=811
13:14:27,763 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:27,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.75. input_tokens=2937, output_tokens=567
13:14:30,908 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:30,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.109999999999673. input_tokens=2937, output_tokens=956
13:14:32,185 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:32,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.1869999999999. input_tokens=2936, output_tokens=645
13:14:32,784 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:32,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.84400000000005. input_tokens=2936, output_tokens=621
13:14:34,701 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:34,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.09400000000005. input_tokens=2936, output_tokens=671
13:14:36,465 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:36,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.54699999999957. input_tokens=2938, output_tokens=765
13:14:37,117 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:37,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.32800000000043. input_tokens=34, output_tokens=725
13:14:37,599 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:37,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.25. input_tokens=2936, output_tokens=910
13:14:40,215 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:40,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.29699999999957. input_tokens=2936, output_tokens=1525
13:14:40,747 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:40,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.453999999999724. input_tokens=2936, output_tokens=765
13:14:42,161 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:42,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.609999999999673. input_tokens=34, output_tokens=803
13:14:43,112 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:43,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.65599999999995. input_tokens=2936, output_tokens=699
13:14:43,230 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:43,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.29700000000048. input_tokens=34, output_tokens=820
13:14:44,789 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:44,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.390000000000327. input_tokens=34, output_tokens=660
13:14:46,244 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:46,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.6880000000001. input_tokens=2936, output_tokens=581
13:14:46,517 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:46,518 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.32800000000043. input_tokens=34, output_tokens=438
13:14:46,633 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:46,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.59400000000005. input_tokens=2936, output_tokens=533
13:14:47,331 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:47,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.657000000000153. input_tokens=1867, output_tokens=676
13:14:49,3 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:49,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.96900000000005. input_tokens=34, output_tokens=1012
13:14:49,499 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:49,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.59400000000005. input_tokens=34, output_tokens=829
13:14:50,709 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:50,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.78200000000015. input_tokens=2936, output_tokens=952
13:14:51,373 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:51,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.59400000000005. input_tokens=34, output_tokens=454
13:14:53,275 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:53,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.71900000000005. input_tokens=34, output_tokens=632
13:14:53,782 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:53,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.3130000000001. input_tokens=2938, output_tokens=583
13:14:54,367 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:54,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.67200000000048. input_tokens=34, output_tokens=1040
13:14:54,387 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:54,388 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.625. input_tokens=34, output_tokens=1022
13:14:56,955 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:56,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.125. input_tokens=34, output_tokens=823
13:14:58,651 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:58,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.54699999999957. input_tokens=34, output_tokens=598
13:14:58,866 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:14:58,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.625. input_tokens=34, output_tokens=515
13:15:00,277 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:00,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.765999999999622. input_tokens=34, output_tokens=340
13:15:03,309 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:03,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.45299999999952. input_tokens=34, output_tokens=1111
13:15:05,517 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:05,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.04700000000048. input_tokens=34, output_tokens=802
13:15:05,857 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:05,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.21900000000005. input_tokens=34, output_tokens=566
13:15:06,280 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:06,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.484999999999673. input_tokens=34, output_tokens=847
13:15:06,711 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:06,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.0. input_tokens=34, output_tokens=441
13:15:06,755 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:06,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.469000000000051. input_tokens=34, output_tokens=395
13:15:06,868 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:06,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.375. input_tokens=34, output_tokens=461
13:15:07,21 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:07,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.265000000000327. input_tokens=34, output_tokens=990
13:15:07,177 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:07,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.390000000000327. input_tokens=34, output_tokens=518
13:15:08,507 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:08,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.34400000000005. input_tokens=34, output_tokens=1024
13:15:09,768 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:09,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.89000000000033. input_tokens=34, output_tokens=835
13:15:09,804 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:09,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.20300000000043. input_tokens=34, output_tokens=1135
13:15:10,356 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:10,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.23400000000038. input_tokens=34, output_tokens=845
13:15:10,959 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:10,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.735000000000582. input_tokens=34, output_tokens=950
13:15:13,463 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:13,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.092999999999847. input_tokens=34, output_tokens=514
13:15:15,638 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:15,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.9369999999999. input_tokens=34, output_tokens=2228
13:15:16,466 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:16,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.217999999999847. input_tokens=34, output_tokens=735
13:15:18,574 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:18,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.57800000000043. input_tokens=34, output_tokens=921
13:15:22,319 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:22,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.95299999999952. input_tokens=34, output_tokens=506
13:15:32,78 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:32,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.125. input_tokens=34, output_tokens=1054
13:15:32,191 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:32,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.85899999999947. input_tokens=34, output_tokens=1054
13:15:40,892 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:40,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.5. input_tokens=34, output_tokens=1606
13:15:42,809 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:42,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7969999999995707. input_tokens=140, output_tokens=31
13:15:42,815 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:42,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.844000000000051. input_tokens=172, output_tokens=48
13:15:42,839 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:42,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8429999999998472. input_tokens=169, output_tokens=35
13:15:43,9 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.969000000000051. input_tokens=152, output_tokens=56
13:15:43,63 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.094000000000051. input_tokens=163, output_tokens=53
13:15:43,80 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.110000000000582. input_tokens=145, output_tokens=70
13:15:43,400 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.375. input_tokens=176, output_tokens=68
13:15:43,638 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=214, output_tokens=65
13:15:43,660 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.70299999999952. input_tokens=158, output_tokens=53
13:15:43,743 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7340000000003783. input_tokens=166, output_tokens=36
13:15:43,853 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8590000000003783. input_tokens=167, output_tokens=36
13:15:43,937 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:43,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.905999999999949. input_tokens=145, output_tokens=38
13:15:44,94 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:44,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.108999999999469. input_tokens=162, output_tokens=74
13:15:44,126 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:44,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1409999999996217. input_tokens=158, output_tokens=71
13:15:44,264 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:44,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2340000000003783. input_tokens=183, output_tokens=69
13:15:44,670 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:44,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.686999999999898. input_tokens=159, output_tokens=82
13:15:44,912 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:44,913 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.905999999999949. input_tokens=236, output_tokens=103
13:15:44,926 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:44,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9210000000002765. input_tokens=209, output_tokens=133
13:15:45,1 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:45,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.015999999999622. input_tokens=207, output_tokens=115
13:15:45,40 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:45,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.188000000000102. input_tokens=188, output_tokens=65
13:15:45,209 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:45,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.235000000000582. input_tokens=272, output_tokens=149
13:15:45,690 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:45,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5619999999998981. input_tokens=156, output_tokens=45
13:15:45,900 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:45,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.25. input_tokens=193, output_tokens=56
13:15:46,10 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:46,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.04700000000048. input_tokens=187, output_tokens=112
13:15:46,93 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:46,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.155999999999949. input_tokens=198, output_tokens=55
13:15:46,179 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:46,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9679999999998472. input_tokens=152, output_tokens=31
13:15:46,350 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:46,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.483999999999469. input_tokens=246, output_tokens=60
13:15:46,365 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:46,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.29700000000048. input_tokens=201, output_tokens=74
13:15:46,943 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:46,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.936999999999898. input_tokens=219, output_tokens=86
13:15:46,977 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:46,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.875. input_tokens=197, output_tokens=96
13:15:47,97 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:47,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.280999999999949. input_tokens=229, output_tokens=94
13:15:47,451 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:47,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.469000000000051. input_tokens=392, output_tokens=186
13:15:47,486 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:47,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1410000000005311. input_tokens=155, output_tokens=31
13:15:47,557 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:47,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8909999999996217. input_tokens=159, output_tokens=55
13:15:47,574 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:47,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5630000000001019. input_tokens=183, output_tokens=38
13:15:47,709 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:47,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.67200000000048. input_tokens=151, output_tokens=31
13:15:47,804 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:47,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.717999999999847. input_tokens=299, output_tokens=141
13:15:48,45 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:48,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.030999999999949. input_tokens=225, output_tokens=128
13:15:48,113 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:48,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.17200000000048. input_tokens=182, output_tokens=70
13:15:48,548 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:48,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.280999999999949. input_tokens=159, output_tokens=72
13:15:48,634 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:48,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.609999999999673. input_tokens=314, output_tokens=186
13:15:48,659 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:48,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.844000000000051. input_tokens=217, output_tokens=160
13:15:48,721 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:48,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.717999999999847. input_tokens=247, output_tokens=83
13:15:49,37 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1409999999996217. input_tokens=249, output_tokens=167
13:15:49,84 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.67200000000048. input_tokens=245, output_tokens=139
13:15:49,101 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.733999999999469. input_tokens=213, output_tokens=69
13:15:49,774 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.75. input_tokens=195, output_tokens=125
13:15:49,797 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.217999999999847. input_tokens=178, output_tokens=59
13:15:49,879 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.188000000000102. input_tokens=259, output_tokens=102
13:15:49,886 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.79700000000048. input_tokens=227, output_tokens=118
13:15:49,923 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:49,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.436999999999898. input_tokens=217, output_tokens=92
13:15:50,160 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:50,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.219000000000051. input_tokens=215, output_tokens=103
13:15:50,527 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:50,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.969000000000051. input_tokens=164, output_tokens=81
13:15:50,826 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:50,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.188000000000102. input_tokens=177, output_tokens=62
13:15:50,886 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:50,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.344000000000051. input_tokens=169, output_tokens=59
13:15:51,34 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.375. input_tokens=191, output_tokens=60
13:15:51,252 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4849999999996726. input_tokens=195, output_tokens=48
13:15:51,364 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.655999999999949. input_tokens=266, output_tokens=131
13:15:51,455 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.360000000000582. input_tokens=207, output_tokens=78
13:15:51,552 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.092999999999847. input_tokens=202, output_tokens=65
13:15:51,599 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.875. input_tokens=227, output_tokens=77
13:15:51,704 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.907000000000153. input_tokens=196, output_tokens=107
13:15:51,720 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.686999999999898. input_tokens=184, output_tokens=44
13:15:51,737 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,738 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.844000000000051. input_tokens=202, output_tokens=59
13:15:51,805 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.686999999999898. input_tokens=172, output_tokens=60
13:15:51,908 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:51,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.155999999999949. input_tokens=273, output_tokens=155
13:15:52,16 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:52,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.936999999999898. input_tokens=225, output_tokens=81
13:15:52,75 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:52,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.985000000000582. input_tokens=224, output_tokens=83
13:15:52,636 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:52,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7650000000003274. input_tokens=163, output_tokens=65
13:15:52,727 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:52,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9219999999995707. input_tokens=233, output_tokens=86
13:15:52,804 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:52,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.875. input_tokens=204, output_tokens=56
13:15:52,950 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:52,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.594000000000051. input_tokens=184, output_tokens=32
13:15:53,49 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,50 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.155999999999949. input_tokens=178, output_tokens=51
13:15:53,77 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.17200000000048. input_tokens=189, output_tokens=136
13:15:53,287 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.109999999999673. input_tokens=224, output_tokens=79
13:15:53,479 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9530000000004293. input_tokens=178, output_tokens=53
13:15:53,527 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5. input_tokens=201, output_tokens=65
13:15:53,627 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.640999999999622. input_tokens=186, output_tokens=44
13:15:53,687 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.969000000000051. input_tokens=152, output_tokens=34
13:15:53,879 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1719999999995707. input_tokens=153, output_tokens=57
13:15:53,989 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:53,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8280000000004293. input_tokens=232, output_tokens=70
13:15:54,2 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:54,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9849999999996726. input_tokens=160, output_tokens=27
13:15:54,223 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:54,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.139999999999418. input_tokens=191, output_tokens=30
13:15:55,115 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:55,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.155999999999949. input_tokens=155, output_tokens=49
13:15:55,190 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:55,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.140999999999622. input_tokens=175, output_tokens=122
13:15:55,218 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:55,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.530999999999949. input_tokens=178, output_tokens=44
13:15:55,508 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:55,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8599999999996726. input_tokens=187, output_tokens=57
13:15:55,578 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:55,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.29700000000048. input_tokens=174, output_tokens=62
13:15:55,691 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:55,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.030999999999949. input_tokens=279, output_tokens=125
13:15:55,867 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:55,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2340000000003783. input_tokens=171, output_tokens=61
13:15:56,12 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.765000000000327. input_tokens=217, output_tokens=79
13:15:56,84 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.032000000000153. input_tokens=196, output_tokens=77
13:15:56,108 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,110 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.313000000000102. input_tokens=177, output_tokens=70
13:15:56,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.391000000000531. input_tokens=211, output_tokens=65
13:15:56,120 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.390999999999622. input_tokens=166, output_tokens=86
13:15:56,202 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,203 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.125. input_tokens=199, output_tokens=74
13:15:56,383 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.8289999999997235. input_tokens=208, output_tokens=49
13:15:56,549 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:56,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6710000000002765. input_tokens=162, output_tokens=62
13:15:57,438 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:57,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.436999999999898. input_tokens=189, output_tokens=101
13:15:57,547 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:57,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.640000000000327. input_tokens=296, output_tokens=83
13:15:57,566 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:57,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.061999999999898. input_tokens=196, output_tokens=65
13:15:57,748 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:57,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8909999999996217. input_tokens=187, output_tokens=54
13:15:57,796 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:57,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6710000000002765. input_tokens=203, output_tokens=48
13:15:57,852 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:57,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.014999999999418. input_tokens=231, output_tokens=133
13:15:58,212 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.733999999999469. input_tokens=166, output_tokens=95
13:15:58,252 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6719999999995707. input_tokens=207, output_tokens=61
13:15:58,287 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.063000000000102. input_tokens=154, output_tokens=57
13:15:58,344 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.233999999999469. input_tokens=167, output_tokens=71
13:15:58,466 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.45299999999952. input_tokens=187, output_tokens=53
13:15:58,520 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.969000000000051. input_tokens=200, output_tokens=64
13:15:58,773 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.655999999999949. input_tokens=210, output_tokens=58
13:15:58,899 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.905999999999949. input_tokens=176, output_tokens=96
13:15:58,911 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.219000000000051. input_tokens=208, output_tokens=103
13:15:58,972 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.889999999999418. input_tokens=188, output_tokens=80
13:15:58,986 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:58,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.453000000000429. input_tokens=168, output_tokens=145
13:15:59,36 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:59,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.57799999999952. input_tokens=173, output_tokens=30
13:15:59,385 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:59,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.17200000000048. input_tokens=210, output_tokens=137
13:15:59,546 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:59,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9840000000003783. input_tokens=237, output_tokens=48
13:15:59,923 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:15:59,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5460000000002765. input_tokens=223, output_tokens=74
13:16:00,83 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:00,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.282000000000153. input_tokens=180, output_tokens=61
13:16:00,316 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:00,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.561999999999898. input_tokens=286, output_tokens=89
13:16:00,388 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:00,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.280999999999949. input_tokens=228, output_tokens=104
13:16:00,499 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:00,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9539999999997235. input_tokens=264, output_tokens=70
13:16:00,772 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:00,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.25. input_tokens=251, output_tokens=71
13:16:00,955 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:00,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.735000000000582. input_tokens=266, output_tokens=89
13:16:01,158 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.905999999999949. input_tokens=193, output_tokens=84
13:16:01,206 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.360000000000582. input_tokens=226, output_tokens=88
13:16:01,288 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.313000000000102. input_tokens=220, output_tokens=51
13:16:01,303 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5. input_tokens=284, output_tokens=63
13:16:01,315 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.719000000000051. input_tokens=247, output_tokens=157
13:16:01,529 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.094000000000051. input_tokens=205, output_tokens=66
13:16:01,542 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.155999999999949. input_tokens=205, output_tokens=42
13:16:01,741 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.391000000000531. input_tokens=219, output_tokens=81
13:16:01,959 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:01,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.766000000000531. input_tokens=221, output_tokens=108
13:16:02,60 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:02,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=200, output_tokens=37
13:16:02,226 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:02,228 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.186999999999898. input_tokens=284, output_tokens=114
13:16:02,572 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:02,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.57799999999952. input_tokens=283, output_tokens=64
13:16:02,715 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:02,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.639999999999418. input_tokens=208, output_tokens=101
13:16:02,818 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:02,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.046999999999571. input_tokens=251, output_tokens=88
13:16:03,38 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4849999999996726. input_tokens=185, output_tokens=116
13:16:03,126 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.844000000000051. input_tokens=237, output_tokens=70
13:16:03,157 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.20299999999952. input_tokens=169, output_tokens=70
13:16:03,419 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.953000000000429. input_tokens=256, output_tokens=127
13:16:03,771 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.844000000000051. input_tokens=165, output_tokens=88
13:16:03,773 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2650000000003274. input_tokens=210, output_tokens=106
13:16:03,832 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.938000000000102. input_tokens=244, output_tokens=97
13:16:03,919 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:03,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.530999999999949. input_tokens=227, output_tokens=105
13:16:04,9 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:04,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.109000000000378. input_tokens=207, output_tokens=119
13:16:05,842 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:05,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.07799999999952. input_tokens=179, output_tokens=73
13:16:06,109 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:06,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.905999999999949. input_tokens=262, output_tokens=161
13:16:06,550 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
13:16:06,550 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:16:06,559 datashaper.workflow.workflow INFO executing verb create_final_entities
13:16:06,668 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
13:16:06,937 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
13:16:06,938 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:16:06,948 datashaper.workflow.workflow INFO executing verb create_final_nodes
13:16:07,584 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
13:16:07,794 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
13:16:07,794 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:16:07,806 datashaper.workflow.workflow INFO executing verb create_final_communities
13:16:08,38 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
13:16:08,309 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
13:16:08,310 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:16:08,311 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
13:16:08,375 datashaper.workflow.workflow INFO executing verb create_final_relationships
13:16:08,500 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
13:16:08,708 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
13:16:08,709 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:16:08,710 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:16:08,716 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:16:08,735 datashaper.workflow.workflow INFO executing verb create_final_text_units
13:16:08,762 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
13:16:09,28 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_communities', 'create_final_relationships', 'create_final_nodes']
13:16:09,28 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
13:16:09,34 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:16:09,40 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
13:16:09,60 datashaper.workflow.workflow INFO executing verb create_final_community_reports
13:16:09,83 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=2 => 42
13:16:09,122 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 189
13:16:09,277 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 398
13:16:31,677 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:31,679 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:16:31,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.265000000000327. input_tokens=2361, output_tokens=643
13:16:34,697 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:34,699 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:16:34,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.313000000000102. input_tokens=2153, output_tokens=664
13:16:35,226 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:35,227 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:16:35,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.842999999999847. input_tokens=2691, output_tokens=692
13:16:35,429 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:35,430 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:16:35,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.046000000000276. input_tokens=2805, output_tokens=649
13:16:51,258 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:51,263 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:16:51,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.875. input_tokens=4088, output_tokens=960
13:16:54,145 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:16:54,146 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:16:54,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 44.75. input_tokens=3373, output_tokens=678
13:17:09,220 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:09,223 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:09,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 14.967999999999847. input_tokens=2175, output_tokens=616
13:17:10,457 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:10,459 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:10,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.25. input_tokens=2094, output_tokens=762
13:17:11,391 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:11,392 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:11,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.10900000000038. input_tokens=2077, output_tokens=608
13:17:11,531 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:11,533 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:11,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.234999999999673. input_tokens=2532, output_tokens=612
13:17:11,994 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:11,995 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:11,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.703999999999724. input_tokens=2220, output_tokens=576
13:17:12,113 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:12,115 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:12,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.84400000000005. input_tokens=2291, output_tokens=716
13:17:13,973 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:13,975 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:13,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.764999999999418. input_tokens=2095, output_tokens=587
13:17:14,82 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:14,86 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:14,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.82800000000043. input_tokens=2925, output_tokens=713
13:17:14,213 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:14,215 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:14,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.03099999999995. input_tokens=2106, output_tokens=502
13:17:14,512 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:14,514 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:14,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.311999999999898. input_tokens=2903, output_tokens=759
13:17:16,2 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:16,6 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:16,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.813000000000102. input_tokens=2309, output_tokens=716
13:17:16,501 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:16,504 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:16,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.328999999999724. input_tokens=3572, output_tokens=701
13:17:17,523 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:17,526 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:17,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.23400000000038. input_tokens=2212, output_tokens=576
13:17:17,964 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:17,965 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:17,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.75. input_tokens=2173, output_tokens=572
13:17:18,348 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:18,349 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:18,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.07799999999952. input_tokens=3204, output_tokens=699
13:17:19,212 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:19,214 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:19,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.98399999999947. input_tokens=2143, output_tokens=634
13:17:20,571 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:20,575 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:20,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.34400000000005. input_tokens=3075, output_tokens=747
13:17:21,670 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:21,674 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:21,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.40599999999995. input_tokens=2147, output_tokens=566
13:17:23,602 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:23,605 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:23,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.39100000000053. input_tokens=2931, output_tokens=801
13:17:25,925 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:25,929 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:25,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.70300000000043. input_tokens=2177, output_tokens=601
13:17:26,591 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:26,593 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:26,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.3119999999999. input_tokens=3441, output_tokens=832
13:17:27,19 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:27,22 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:27,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.484000000000378. input_tokens=2078, output_tokens=581
13:17:27,399 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:27,402 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:27,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.14099999999962. input_tokens=2279, output_tokens=578
13:17:27,460 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:27,465 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:27,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.21799999999985. input_tokens=3398, output_tokens=805
13:17:31,347 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:31,349 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:31,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.15599999999995. input_tokens=5696, output_tokens=695
13:17:31,403 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:31,406 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:31,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.40599999999995. input_tokens=2860, output_tokens=701
13:17:32,182 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:32,186 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:32,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.89099999999962. input_tokens=3474, output_tokens=895
13:17:32,223 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:32,226 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:32,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.70299999999952. input_tokens=2225, output_tokens=634
13:17:34,387 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:34,390 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:34,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.17200000000048. input_tokens=2183, output_tokens=600
13:17:35,634 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:35,635 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:35,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.625. input_tokens=2073, output_tokens=550
13:17:36,248 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:36,251 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:36,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.282000000000153. input_tokens=2065, output_tokens=523
13:17:36,611 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:36,614 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:36,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.07800000000043. input_tokens=2694, output_tokens=676
13:17:36,776 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:36,779 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:36,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 42.54699999999957. input_tokens=3029, output_tokens=706
13:17:37,6 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:37,9 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:37,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.54699999999957. input_tokens=2544, output_tokens=680
13:17:38,399 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:38,404 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:38,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.90599999999995. input_tokens=2771, output_tokens=874
13:17:38,763 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:38,766 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:38,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.65599999999995. input_tokens=2272, output_tokens=622
13:17:38,937 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:38,943 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:38,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.54699999999957. input_tokens=2578, output_tokens=673
13:17:39,440 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:39,443 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:39,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.35899999999947. input_tokens=2667, output_tokens=674
13:17:47,363 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:47,366 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:47,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.14100000000053. input_tokens=2744, output_tokens=738
13:17:49,496 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:49,497 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:49,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.532000000000153. input_tokens=3268, output_tokens=739
13:17:50,192 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:17:50,194 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:17:50,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.84400000000005. input_tokens=3964, output_tokens=683
13:18:06,158 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:06,160 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:06,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.905999999999949. input_tokens=2231, output_tokens=557
13:18:08,799 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:08,801 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:08,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.561999999999898. input_tokens=3818, output_tokens=718
13:18:10,953 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:10,956 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:10,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.67200000000048. input_tokens=2129, output_tokens=711
13:18:11,293 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:11,294 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:11,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.015000000000327. input_tokens=2141, output_tokens=523
13:18:12,302 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:12,304 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:12,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.015000000000327. input_tokens=3129, output_tokens=691
13:18:18,166 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:18,168 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:18,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.95300000000043. input_tokens=7934, output_tokens=919
13:18:19,850 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:19,852 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:19,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.592999999999847. input_tokens=5649, output_tokens=768
13:18:19,862 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:19,865 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:19,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.625. input_tokens=2671, output_tokens=686
13:18:21,367 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:21,369 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:21,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.09400000000005. input_tokens=5861, output_tokens=720
13:18:25,943 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:25,948 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:25,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.67199999999957. input_tokens=3937, output_tokens=868
13:18:28,159 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:28,164 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:28,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.90599999999995. input_tokens=5804, output_tokens=803
13:18:35,727 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:35,732 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:35,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.51600000000053. input_tokens=4905, output_tokens=1012
13:18:36,350 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
13:18:36,352 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
13:18:36,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.04699999999957. input_tokens=4125, output_tokens=1035
13:18:36,368 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
13:18:36,687 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
13:18:36,687 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
13:18:36,710 datashaper.workflow.workflow INFO executing verb create_final_documents
13:18:36,721 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
13:18:36,934 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_community_reports', 'create_final_entities', 'create_final_documents', 'create_final_text_units', 'create_final_relationships']
13:18:36,935 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
13:18:36,950 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:18:36,957 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
13:18:36,964 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
13:18:36,969 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:18:36,992 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
13:18:36,997 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
13:18:36,997 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
13:18:37,4 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
13:18:37,304 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-3: TPM=0, RPM=0
13:18:37,304 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-3: 25
13:18:37,349 graphrag.index.operations.embed_text.strategies.openai INFO embedding 37 inputs via 37 snippets using 6 batches. max_batch_size=16, max_tokens=8191
13:18:37,940 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:37,946 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:37,951 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:37,953 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:37,969 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:37,997 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:38,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9210000000002765. input_tokens=7200, output_tokens=0
13:18:38,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.030999999999949. input_tokens=7200, output_tokens=0
13:18:38,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1089999999994689. input_tokens=7200, output_tokens=0
13:18:38,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2030000000004293. input_tokens=7331, output_tokens=0
13:18:38,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3119999999998981. input_tokens=7200, output_tokens=0
13:18:38,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3909999999996217. input_tokens=7200, output_tokens=0
13:18:39,84 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
13:18:39,116 graphrag.index.operations.embed_text.strategies.openai INFO embedding 398 inputs via 398 snippets using 25 batches. max_batch_size=16, max_tokens=8191
13:18:39,404 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,405 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,410 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,424 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,430 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,434 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,854 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,857 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,932 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,959 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,989 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:39,990 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,51 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,68 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,128 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,148 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,150 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,166 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,169 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,187 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,188 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,189 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,189 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,250 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:40,689 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:42,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.0780000000004293. input_tokens=744, output_tokens=0
13:18:42,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.6400000000003274. input_tokens=815, output_tokens=0
13:18:43,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.905999999999949. input_tokens=549, output_tokens=0
13:18:43,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.140000000000327. input_tokens=701, output_tokens=0
13:18:43,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.375. input_tokens=709, output_tokens=0
13:18:43,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.609000000000378. input_tokens=600, output_tokens=0
13:18:43,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.813000000000102. input_tokens=938, output_tokens=0
13:18:44,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.061999999999898. input_tokens=759, output_tokens=0
13:18:44,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.342999999999847. input_tokens=387, output_tokens=0
13:18:44,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.563000000000102. input_tokens=491, output_tokens=0
13:18:44,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.796999999999571. input_tokens=530, output_tokens=0
13:18:45,170 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.015000000000327. input_tokens=610, output_tokens=0
13:18:45,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.265999999999622. input_tokens=733, output_tokens=0
13:18:45,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.5. input_tokens=1171, output_tokens=0
13:18:45,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.75. input_tokens=478, output_tokens=0
13:18:46,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.984000000000378. input_tokens=768, output_tokens=0
13:18:46,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 7.25. input_tokens=865, output_tokens=0
13:18:46,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 7.453000000000429. input_tokens=561, output_tokens=0
13:18:46,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 7.686999999999898. input_tokens=346, output_tokens=0
13:18:47,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 7.905999999999949. input_tokens=871, output_tokens=0
13:18:47,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 8.140999999999622. input_tokens=692, output_tokens=0
13:18:47,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 8.42199999999957. input_tokens=392, output_tokens=0
13:18:47,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 8.640999999999622. input_tokens=779, output_tokens=0
13:18:48,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 8.890000000000327. input_tokens=651, output_tokens=0
13:18:48,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 9.171000000000276. input_tokens=1185, output_tokens=0
13:18:48,448 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
13:18:48,503 graphrag.index.operations.embed_text.strategies.openai INFO embedding 60 inputs via 60 snippets using 5 batches. max_batch_size=16, max_tokens=8191
13:18:48,799 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:48,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3280000000004293. input_tokens=826, output_tokens=0
13:18:49,35 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:49,70 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:49,76 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:49,81 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
13:18:49,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.094000000000051. input_tokens=8028, output_tokens=0
13:18:49,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3119999999998981. input_tokens=7963, output_tokens=0
13:18:50,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5150000000003274. input_tokens=7686, output_tokens=0
13:18:50,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.75. input_tokens=7997, output_tokens=0
13:18:50,371 graphrag.cli.index INFO All workflows completed successfully.
14:28:14,960 graphrag.cli.index INFO Logging enabled at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\logs\indexing-engine.log
14:28:14,964 graphrag.cli.index INFO Starting pipeline run for: 20241210-142814, dry_run=False
14:28:14,966 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "glm-4-air",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://open.bigmodel.cn/api/paas/v4",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": null,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "embedding-3",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
14:28:14,975 graphrag.index.create_pipeline_config INFO skipping workflows 
14:28:14,975 graphrag.index.run.run INFO Running pipeline
14:28:14,975 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\output
14:28:14,976 graphrag.index.input.load_input INFO loading input from root_dir=input
14:28:14,976 graphrag.index.input.load_input INFO using file storage for input
14:28:14,977 graphrag.index.storage.file_pipeline_storage INFO search D:\GISERR\graduate_study\pytorch\graphrag\ragtest\input for files matching .*\.txt$
14:28:14,978 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
14:28:14,984 graphrag.index.input.text INFO Found 1 files, loading 1
14:28:14,990 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
14:28:14,990 graphrag.index.run.run INFO Final # of rows loaded: 1
14:28:15,179 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
14:28:15,186 datashaper.workflow.workflow INFO executing verb create_base_text_units
14:28:16,214 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
14:28:16,215 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:28:16,222 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
14:28:16,230 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
14:28:16,568 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4-air: TPM=0, RPM=0
14:28:16,569 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4-air: 25
14:28:20,400 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:20,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.75. input_tokens=2936, output_tokens=63
14:28:20,468 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:20,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.8430000000007567. input_tokens=2937, output_tokens=75
14:28:20,536 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:20,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.8910000000014406. input_tokens=2936, output_tokens=70
14:28:20,618 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:20,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.0. input_tokens=2936, output_tokens=106
14:28:21,313 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:21,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.640999999999622. input_tokens=2936, output_tokens=72
14:28:21,484 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:21,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.84400000000096. input_tokens=2936, output_tokens=106
14:28:21,498 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:21,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.890999999999622. input_tokens=2936, output_tokens=75
14:28:21,868 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:21,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.218999999999141. input_tokens=2937, output_tokens=86
14:28:22,167 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:22,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.5460000000002765. input_tokens=2937, output_tokens=96
14:28:22,423 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:22,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.82799999999952. input_tokens=2936, output_tokens=102
14:28:23,177 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:23,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.57799999999952. input_tokens=2936, output_tokens=103
14:28:24,98 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:24,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.515000000001237. input_tokens=2936, output_tokens=89
14:28:24,717 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:24,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.04700000000048. input_tokens=2937, output_tokens=96
14:28:27,670 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:27,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.25. input_tokens=34, output_tokens=164
14:28:28,955 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:28,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.281999999999243. input_tokens=2937, output_tokens=387
14:28:33,420 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:33,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.546000000000276. input_tokens=34, output_tokens=405
14:28:35,335 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:35,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.734999999998763. input_tokens=2936, output_tokens=752
14:28:36,551 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:36,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.90600000000086. input_tokens=2936, output_tokens=975
14:28:37,320 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:37,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.20299999999952. input_tokens=34, output_tokens=456
14:28:37,407 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:37,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.0. input_tokens=34, output_tokens=468
14:28:38,788 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:38,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.09400000000096. input_tokens=2723, output_tokens=623
14:28:39,47 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:39,50 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.57799999999952. input_tokens=34, output_tokens=540
14:28:39,220 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:39,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.60900000000038. input_tokens=2936, output_tokens=630
14:28:39,967 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:39,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.343000000000757. input_tokens=2937, output_tokens=609
14:28:40,915 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:40,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.375. input_tokens=34, output_tokens=544
14:28:41,79 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:41,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.59399999999914. input_tokens=34, output_tokens=697
14:28:42,800 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:42,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.25. input_tokens=34, output_tokens=228
14:28:43,571 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:43,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.23400000000038. input_tokens=34, output_tokens=394
14:28:43,892 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:43,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.28099999999904. input_tokens=2937, output_tokens=454
14:28:44,605 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:44,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.95299999999952. input_tokens=2937, output_tokens=734
14:28:44,798 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:44,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.296000000000276. input_tokens=34, output_tokens=440
14:28:45,258 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:45,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.063000000000102. input_tokens=34, output_tokens=624
14:28:45,406 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:45,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.78100000000086. input_tokens=34, output_tokens=405
14:28:47,773 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:47,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.139999999999418. input_tokens=2936, output_tokens=569
14:28:47,952 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:47,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.281999999999243. input_tokens=2937, output_tokens=870
14:28:48,430 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:48,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.764999999999418. input_tokens=2937, output_tokens=823
14:28:50,32 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:50,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.313000000000102. input_tokens=34, output_tokens=534
14:28:55,855 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:55,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.90600000000086. input_tokens=34, output_tokens=593
14:28:56,638 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:56,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.46899999999914. input_tokens=34, output_tokens=598
14:28:57,480 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:57,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.26599999999962. input_tokens=34, output_tokens=427
14:28:58,881 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:28:58,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.92200000000048. input_tokens=34, output_tokens=582
14:29:00,593 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:00,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.79700000000048. input_tokens=34, output_tokens=735
14:29:01,550 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:01,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.65600000000086. input_tokens=34, output_tokens=448
14:29:05,901 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:05,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.938000000000102. input_tokens=34, output_tokens=460
14:29:06,506 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:06,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.90599999999904. input_tokens=34, output_tokens=610
14:29:07,839 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:07,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.53199999999924. input_tokens=34, output_tokens=1823
14:29:08,964 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:08,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.53100000000086. input_tokens=34, output_tokens=784
14:29:18,419 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:18,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.639999999999418. input_tokens=34, output_tokens=439
14:29:20,222 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:20,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7180000000007567. input_tokens=142, output_tokens=28
14:29:20,465 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:20,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9530000000013388. input_tokens=180, output_tokens=89
14:29:20,822 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:20,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3439999999991414. input_tokens=167, output_tokens=60
14:29:20,946 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:20,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3909999999996217. input_tokens=164, output_tokens=77
14:29:20,958 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:20,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4219999999986612. input_tokens=183, output_tokens=59
14:29:21,1 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.438000000000102. input_tokens=180, output_tokens=50
14:29:21,145 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6559999999990396. input_tokens=185, output_tokens=71
14:29:21,211 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.688000000000102. input_tokens=183, output_tokens=78
14:29:21,221 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,222 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7030000000013388. input_tokens=173, output_tokens=64
14:29:21,273 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7339999999985594. input_tokens=141, output_tokens=33
14:29:21,413 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8440000000009604. input_tokens=221, output_tokens=90
14:29:21,424 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.875. input_tokens=207, output_tokens=47
14:29:21,561 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:21,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0309999999990396. input_tokens=180, output_tokens=60
14:29:22,68 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5309999999990396. input_tokens=193, output_tokens=50
14:29:22,484 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2659999999996217. input_tokens=192, output_tokens=70
14:29:22,492 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5630000000001019. input_tokens=174, output_tokens=63
14:29:22,554 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9840000000003783. input_tokens=291, output_tokens=126
14:29:22,828 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6880000000001019. input_tokens=177, output_tokens=55
14:29:22,903 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9060000000008586. input_tokens=176, output_tokens=59
14:29:22,952 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0. input_tokens=171, output_tokens=50
14:29:22,958 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:22,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.438000000000102. input_tokens=173, output_tokens=56
14:29:23,130 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9069999999992433. input_tokens=179, output_tokens=66
14:29:23,252 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.75. input_tokens=214, output_tokens=57
14:29:23,450 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.25. input_tokens=169, output_tokens=55
14:29:23,542 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.0. input_tokens=416, output_tokens=159
14:29:23,628 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,630 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.063000000000102. input_tokens=184, output_tokens=66
14:29:23,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.0789999999997235. input_tokens=398, output_tokens=179
14:29:23,848 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.343000000000757. input_tokens=327, output_tokens=137
14:29:23,997 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:23,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5939999999991414. input_tokens=220, output_tokens=72
14:29:24,336 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:24,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.859999999998763. input_tokens=181, output_tokens=81
14:29:24,717 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:24,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.890000000001237. input_tokens=177, output_tokens=45
14:29:24,730 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:24,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.20299999999952. input_tokens=176, output_tokens=62
14:29:24,880 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:24,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4539999999997235. input_tokens=224, output_tokens=116
14:29:24,894 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:24,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.375. input_tokens=474, output_tokens=191
14:29:25,59 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,60 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.561999999999898. input_tokens=424, output_tokens=284
14:29:25,393 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.75. input_tokens=201, output_tokens=66
14:29:25,487 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.21900000000096. input_tokens=301, output_tokens=232
14:29:25,569 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.061999999999898. input_tokens=212, output_tokens=48
14:29:25,656 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2030000000013388. input_tokens=222, output_tokens=50
14:29:25,826 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7659999999996217. input_tokens=291, output_tokens=118
14:29:25,845 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.890000000001237. input_tokens=202, output_tokens=81
14:29:25,870 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:25,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9689999999991414. input_tokens=273, output_tokens=121
14:29:26,33 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:26,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0780000000013388. input_tokens=233, output_tokens=105
14:29:26,51 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:26,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.561999999999898. input_tokens=436, output_tokens=131
14:29:26,119 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:26,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0. input_tokens=172, output_tokens=77
14:29:26,222 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:26,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.67200000000048. input_tokens=219, output_tokens=69
14:29:26,287 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:26,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.79700000000048. input_tokens=237, output_tokens=122
14:29:26,366 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:26,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.813000000000102. input_tokens=368, output_tokens=127
14:29:26,514 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:26,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.264999999999418. input_tokens=332, output_tokens=94
14:29:27,296 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:27,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6710000000002765. input_tokens=240, output_tokens=100
14:29:29,37 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:29,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.203000000001339. input_tokens=225, output_tokens=182
14:29:37,184 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:37,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 18.64099999999962. input_tokens=166, output_tokens=80
14:29:37,496 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
14:29:37,497 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:29:37,505 datashaper.workflow.workflow INFO executing verb create_final_entities
14:29:37,540 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
14:29:37,821 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
14:29:37,822 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:29:37,834 datashaper.workflow.workflow INFO executing verb create_final_nodes
14:29:37,917 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
14:29:38,127 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
14:29:38,128 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:29:38,141 datashaper.workflow.workflow INFO executing verb create_final_communities
14:29:38,193 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
14:29:38,400 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
14:29:38,401 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:29:38,402 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
14:29:38,459 datashaper.workflow.workflow INFO executing verb create_final_relationships
14:29:38,480 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
14:29:38,768 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
14:29:38,768 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:29:38,770 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
14:29:38,775 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
14:29:38,795 datashaper.workflow.workflow INFO executing verb create_final_text_units
14:29:38,819 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
14:29:39,79 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_communities', 'create_final_nodes']
14:29:39,80 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
14:29:39,87 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
14:29:39,92 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
14:29:39,111 datashaper.workflow.workflow INFO executing verb create_final_community_reports
14:29:39,120 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 30
14:29:39,172 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 72
14:29:55,612 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:55,614 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:29:55,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.375. input_tokens=2099, output_tokens=522
14:29:56,261 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:56,263 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:29:56,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.04699999999866. input_tokens=2278, output_tokens=925
14:29:57,122 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:29:57,124 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:29:57,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.89099999999962. input_tokens=2156, output_tokens=616
14:30:10,459 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:30:10,461 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:30:10,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.234999999998763. input_tokens=2603, output_tokens=682
14:30:13,636 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:30:13,638 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:30:13,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.42199999999866. input_tokens=4033, output_tokens=841
14:30:22,360 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:30:22,362 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:30:22,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.125. input_tokens=5061, output_tokens=871
14:30:47,274 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:30:47,276 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:30:47,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.875. input_tokens=2966, output_tokens=689
14:30:55,583 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:30:55,585 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:30:55,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.1880000000001. input_tokens=5973, output_tokens=742
14:30:56,847 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:30:56,848 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:30:56,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.45300000000134. input_tokens=2583, output_tokens=757
14:31:03,677 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
14:31:03,679 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
14:31:03,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.29600000000028. input_tokens=4798, output_tokens=867
14:31:03,694 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
14:31:03,914 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
14:31:03,915 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
14:31:03,937 datashaper.workflow.workflow INFO executing verb create_final_documents
14:31:03,950 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
14:31:04,152 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_relationships', 'create_final_text_units', 'create_final_entities', 'create_final_community_reports', 'create_final_documents']
14:31:04,153 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
14:31:04,160 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
14:31:04,164 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
14:31:04,171 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
14:31:04,176 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
14:31:04,200 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
14:31:04,204 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
14:31:04,204 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
14:31:04,210 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
14:31:04,506 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-3: TPM=0, RPM=0
14:31:04,506 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-3: 25
14:31:04,515 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 10 snippets using 1 batches. max_batch_size=16, max_tokens=8191
14:31:05,65 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:05,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8280000000013388. input_tokens=5920, output_tokens=0
14:31:05,701 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
14:31:05,719 graphrag.index.operations.embed_text.strategies.openai INFO embedding 24 inputs via 24 snippets using 4 batches. max_batch_size=16, max_tokens=8191
14:31:05,968 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4069999999992433. input_tokens=7200, output_tokens=0
14:31:06,157 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,158 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,165 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5939999999991414. input_tokens=7200, output_tokens=0
14:31:06,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6869999999998981. input_tokens=6986, output_tokens=0
14:31:06,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7969999999986612. input_tokens=7200, output_tokens=0
14:31:06,563 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
14:31:06,572 graphrag.index.operations.embed_text.strategies.openai INFO embedding 72 inputs via 72 snippets using 5 batches. max_batch_size=16, max_tokens=8191
14:31:06,758 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,772 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,776 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,777 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:06,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3900000000012369. input_tokens=301, output_tokens=0
14:31:06,978 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
14:31:07,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.625. input_tokens=1108, output_tokens=0
14:31:07,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8430000000007567. input_tokens=848, output_tokens=0
14:31:07,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0930000000007567. input_tokens=421, output_tokens=0
14:31:07,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3280000000013388. input_tokens=1386, output_tokens=0
14:31:08,5 graphrag.cli.index INFO All workflows completed successfully.
21:10:03,533 graphrag.cli.index INFO Logging enabled at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\logs\indexing-engine.log
21:10:03,537 graphrag.cli.index INFO Starting pipeline run for: 20241210-211003, dry_run=False
21:10:03,540 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "glm-4-air",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://open.bigmodel.cn/api/paas/v4",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": null,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "D:\\GISERR\\graduate_study\\pytorch\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "embedding-3",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "glm-4-air",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://open.bigmodel.cn/api/paas/v4",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
21:10:03,549 graphrag.index.create_pipeline_config INFO skipping workflows 
21:10:03,549 graphrag.index.run.run INFO Running pipeline
21:10:03,549 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at D:\GISERR\graduate_study\pytorch\graphrag\ragtest\output
21:10:03,550 graphrag.index.input.load_input INFO loading input from root_dir=input
21:10:03,550 graphrag.index.input.load_input INFO using file storage for input
21:10:03,553 graphrag.index.storage.file_pipeline_storage INFO search D:\GISERR\graduate_study\pytorch\graphrag\ragtest\input for files matching .*\.txt$
21:10:03,553 graphrag.index.input.text INFO found text files from input, found [('emergency.txt', {}), ('flood.txt', {}), ('test.txt', {})]
21:10:03,562 graphrag.index.input.text INFO Found 3 files, loading 3
21:10:03,566 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
21:10:03,567 graphrag.index.run.run INFO Final # of rows loaded: 3
21:10:03,768 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:10:03,774 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:10:05,311 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:10:05,313 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
21:10:05,320 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:10:05,333 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
21:10:05,723 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4-air: TPM=0, RPM=0
21:10:05,723 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4-air: 25
21:10:09,346 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:09,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.5319999999992433. input_tokens=2937, output_tokens=79
21:10:10,44 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.218000000000757. input_tokens=2937, output_tokens=75
21:10:10,52 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.234000000004016. input_tokens=2936, output_tokens=88
21:10:10,205 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.406999999999243. input_tokens=2938, output_tokens=78
21:10:10,450 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.671999999998661. input_tokens=2937, output_tokens=76
21:10:10,519 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.73399999999674. input_tokens=2936, output_tokens=86
21:10:10,667 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.843000000000757. input_tokens=2938, output_tokens=88
21:10:10,770 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.0. input_tokens=2937, output_tokens=68
21:10:10,781 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:10,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.922000000005937. input_tokens=1934, output_tokens=85
21:10:11,315 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:11,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.5. input_tokens=2937, output_tokens=89
21:10:12,191 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:12,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.328000000001339. input_tokens=2936, output_tokens=120
21:10:13,120 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:13,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.3289999999979045. input_tokens=2935, output_tokens=121
21:10:13,126 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:13,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.3439999999973224. input_tokens=2937, output_tokens=118
21:10:13,546 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:13,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.718000000000757. input_tokens=2936, output_tokens=85
21:10:14,328 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:14,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.485000000000582. input_tokens=2936, output_tokens=77
21:10:14,652 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:14,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.875. input_tokens=2937, output_tokens=106
21:10:15,915 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:15,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.109000000004016. input_tokens=2285, output_tokens=114
21:10:18,901 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:18,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.06300000000192. input_tokens=2936, output_tokens=350
21:10:25,333 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:25,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.51600000000326. input_tokens=2935, output_tokens=680
21:10:27,146 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:27,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.281000000002678. input_tokens=2935, output_tokens=643
21:10:29,757 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:29,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.98399999999674. input_tokens=2937, output_tokens=998
21:10:30,183 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:30,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.31199999999808. input_tokens=2937, output_tokens=752
21:10:30,944 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:30,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.43800000000192. input_tokens=2937, output_tokens=971
21:10:33,110 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:33,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.79699999999866. input_tokens=2935, output_tokens=518
21:10:35,993 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:36,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.218999999997322. input_tokens=2936, output_tokens=424
21:10:37,510 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:37,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.656000000002678. input_tokens=2937, output_tokens=696
21:10:38,813 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:38,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.625. input_tokens=2937, output_tokens=932
21:10:39,810 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:39,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.60899999999674. input_tokens=2937, output_tokens=953
21:10:40,359 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:40,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.593999999997322. input_tokens=2936, output_tokens=519
21:10:41,53 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:41,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5310000000026776. input_tokens=34, output_tokens=83
21:10:41,348 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:41,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.014999999999418. input_tokens=2937, output_tokens=685
21:10:41,628 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:41,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.578999999997905. input_tokens=2935, output_tokens=834
21:10:43,460 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:43,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.79699999999866. input_tokens=2936, output_tokens=738
21:10:44,914 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:44,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.547000000005937. input_tokens=34, output_tokens=85
21:10:45,241 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:45,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.312999999994645. input_tokens=2935, output_tokens=747
21:10:45,591 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:45,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.82800000000134. input_tokens=2936, output_tokens=476
21:10:46,421 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:46,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.296000000002095. input_tokens=2936, output_tokens=1055
21:10:46,943 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:46,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.79699999999866. input_tokens=2936, output_tokens=1302
21:10:47,76 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:47,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.32800000000134. input_tokens=2937, output_tokens=1540
21:10:47,317 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:47,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.9689999999973224. input_tokens=34, output_tokens=152
21:10:47,940 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:47,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.48399999999674. input_tokens=2936, output_tokens=805
21:10:48,136 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:48,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.514999999999418. input_tokens=34, output_tokens=83
21:10:48,248 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:48,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.796999999998661. input_tokens=34, output_tokens=77
21:10:48,554 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:48,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.0. input_tokens=2936, output_tokens=744
21:10:48,651 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:48,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.547000000005937. input_tokens=2937, output_tokens=402
21:10:48,784 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:48,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.969000000004598. input_tokens=34, output_tokens=94
21:10:48,809 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:48,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.48399999999674. input_tokens=2937, output_tokens=781
21:10:50,303 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:50,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.389999999999418. input_tokens=34, output_tokens=92
21:10:50,343 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:50,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.67199999999866. input_tokens=2936, output_tokens=664
21:10:52,173 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:52,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.937000000005355. input_tokens=34, output_tokens=86
21:10:53,595 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:53,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.453000000001339. input_tokens=34, output_tokens=85
21:10:53,978 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:53,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.375. input_tokens=34, output_tokens=106
21:10:53,986 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:53,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.98399999999674. input_tokens=2937, output_tokens=651
21:10:54,275 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:54,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.344000000004598. input_tokens=34, output_tokens=153
21:10:54,897 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:54,898 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.828000000001339. input_tokens=34, output_tokens=164
21:10:56,65 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:56,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.875. input_tokens=2935, output_tokens=896
21:10:57,955 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:10:57,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.04699999999866. input_tokens=2938, output_tokens=419
21:11:00,255 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:00,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.09399999999732. input_tokens=2939, output_tokens=894
21:11:00,812 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:00,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.85899999999674. input_tokens=2937, output_tokens=876
21:11:01,447 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:01,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.375. input_tokens=34, output_tokens=197
21:11:01,533 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:01,535 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.76600000000326. input_tokens=2937, output_tokens=1124
21:11:05,911 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:05,913 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.86000000000058. input_tokens=2937, output_tokens=1052
21:11:06,392 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:06,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.406000000002678. input_tokens=34, output_tokens=655
21:11:06,488 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:06,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.937999999994645. input_tokens=34, output_tokens=544
21:11:08,175 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:08,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.75. input_tokens=34, output_tokens=567
21:11:10,85 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:10,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.42199999999866. input_tokens=34, output_tokens=574
21:11:12,848 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:12,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.31199999999808. input_tokens=34, output_tokens=367
21:11:13,307 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:13,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.718999999997322. input_tokens=34, output_tokens=621
21:11:13,422 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:13,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.531000000002678. input_tokens=2937, output_tokens=700
21:11:14,456 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:14,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.51600000000326. input_tokens=34, output_tokens=754
21:11:14,665 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:14,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.5940000000046. input_tokens=34, output_tokens=705
21:11:14,936 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:14,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.625. input_tokens=34, output_tokens=633
21:11:16,573 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:16,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.32800000000134. input_tokens=34, output_tokens=490
21:11:18,777 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:18,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.9690000000046. input_tokens=34, output_tokens=571
21:11:21,121 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:21,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.781999999999243. input_tokens=34, output_tokens=488
21:11:24,767 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:24,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.48399999999674. input_tokens=34, output_tokens=482
21:11:25,451 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:25,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.9690000000046. input_tokens=34, output_tokens=394
21:11:26,93 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:26,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.28100000000268. input_tokens=34, output_tokens=630
21:11:28,346 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:28,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.17199999999866. input_tokens=34, output_tokens=552
21:11:28,627 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:28,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.25. input_tokens=2937, output_tokens=1167
21:11:28,894 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:28,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.07800000000134. input_tokens=2936, output_tokens=707
21:11:29,149 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:29,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.39100000000326. input_tokens=2936, output_tokens=91
21:11:29,514 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:29,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.73399999999674. input_tokens=34, output_tokens=1074
21:11:30,17 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:30,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.93699999999808. input_tokens=34, output_tokens=554
21:11:30,117 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:30,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.81300000000192. input_tokens=2937, output_tokens=536
21:11:30,405 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:30,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.01600000000326. input_tokens=34, output_tokens=787
21:11:31,258 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:31,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.796999999998661. input_tokens=2938, output_tokens=120
21:11:34,697 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:34,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.23399999999674. input_tokens=34, output_tokens=596
21:11:34,917 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:34,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.671000000002095. input_tokens=34, output_tokens=836
21:11:34,941 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:34,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.765999999995984. input_tokens=2936, output_tokens=1196
21:11:39,23 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:39,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.17199999999866. input_tokens=2937, output_tokens=884
21:11:40,489 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:40,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.54699999999866. input_tokens=34, output_tokens=552
21:11:41,113 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:41,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.327999999994063. input_tokens=34, output_tokens=659
21:11:42,254 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:42,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.313000000001921. input_tokens=34, output_tokens=285
21:11:44,56 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:44,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.75. input_tokens=34, output_tokens=1884
21:11:44,461 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:44,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.48500000000058. input_tokens=34, output_tokens=1907
21:11:45,441 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:45,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.405999999995402. input_tokens=34, output_tokens=85
21:11:47,270 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:47,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.75. input_tokens=34, output_tokens=772
21:11:48,18 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:48,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.67199999999866. input_tokens=1867, output_tokens=552
21:11:48,592 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:48,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.13999999999942. input_tokens=34, output_tokens=1039
21:11:48,944 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:48,946 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.81199999999808. input_tokens=2936, output_tokens=585
21:11:50,115 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:50,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.45299999999406. input_tokens=2937, output_tokens=756
21:11:50,815 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:50,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.92199999999866. input_tokens=34, output_tokens=425
21:11:53,205 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:53,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.07800000000134. input_tokens=34, output_tokens=668
21:11:53,212 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:53,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.81199999999808. input_tokens=34, output_tokens=649
21:11:55,176 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:55,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.25. input_tokens=34, output_tokens=479
21:11:56,103 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:56,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.687999999994645. input_tokens=34, output_tokens=571
21:11:56,228 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:56,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.5309999999954. input_tokens=34, output_tokens=574
21:11:57,342 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:57,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.76499999999942. input_tokens=2937, output_tokens=796
21:11:57,969 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:11:57,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.95300000000134. input_tokens=34, output_tokens=922
21:12:05,969 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:05,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.01499999999942. input_tokens=34, output_tokens=2797
21:12:05,990 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:05,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.73399999999674. input_tokens=34, output_tokens=755
21:12:06,528 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:06,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.422000000005937. input_tokens=34, output_tokens=928
21:12:08,821 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:08,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.18699999999808. input_tokens=34, output_tokens=1174
21:12:11,10 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:11,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.85899999999674. input_tokens=34, output_tokens=821
21:12:11,505 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:11,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 65.59399999999732. input_tokens=34, output_tokens=1231
21:12:14,613 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:14,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.48399999999674. input_tokens=34, output_tokens=891
21:12:15,996 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:15,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.968999999997322. input_tokens=34, output_tokens=536
21:12:17,220 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:17,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.875. input_tokens=34, output_tokens=330
21:12:17,413 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:17,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.31300000000192. input_tokens=34, output_tokens=894
21:12:17,466 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:17,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.514999999999418. input_tokens=34, output_tokens=677
21:12:25,922 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:25,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.437000000005355. input_tokens=34, output_tokens=1148
21:12:27,757 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:27,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6880000000019209. input_tokens=179, output_tokens=62
21:12:28,38 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9380000000019209. input_tokens=155, output_tokens=30
21:12:28,60 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9839999999967404. input_tokens=186, output_tokens=42
21:12:28,90 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9530000000013388. input_tokens=199, output_tokens=55
21:12:28,243 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2039999999979045. input_tokens=224, output_tokens=66
21:12:28,328 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2030000000013388. input_tokens=151, output_tokens=34
21:12:28,368 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2339999999967404. input_tokens=163, output_tokens=59
21:12:28,496 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4069999999992433. input_tokens=157, output_tokens=47
21:12:28,551 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4840000000040163. input_tokens=176, output_tokens=36
21:12:28,595 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5. input_tokens=143, output_tokens=50
21:12:28,810 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.75. input_tokens=171, output_tokens=98
21:12:28,973 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:28,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9060000000026776. input_tokens=198, output_tokens=53
21:12:29,430 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:29,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6710000000020955. input_tokens=154, output_tokens=31
21:12:29,474 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:29,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.375. input_tokens=172, output_tokens=96
21:12:29,555 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:29,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4530000000013388. input_tokens=158, output_tokens=66
21:12:29,609 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:29,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5. input_tokens=206, output_tokens=75
21:12:29,645 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:29,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.514999999999418. input_tokens=185, output_tokens=70
21:12:29,744 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:29,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6719999999986612. input_tokens=167, output_tokens=70
21:12:29,868 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:29,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7659999999959837. input_tokens=159, output_tokens=42
21:12:30,417 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:30,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.2810000000026776. input_tokens=175, output_tokens=79
21:12:30,557 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:30,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.311999999998079. input_tokens=180, output_tokens=41
21:12:30,860 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:30,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.780999999995402. input_tokens=195, output_tokens=94
21:12:30,917 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:30,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.375. input_tokens=163, output_tokens=50
21:12:30,933 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:30,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3439999999973224. input_tokens=147, output_tokens=49
21:12:31,519 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:31,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.875. input_tokens=153, output_tokens=53
21:12:31,578 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:31,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5469999999986612. input_tokens=201, output_tokens=87
21:12:31,834 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:31,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9690000000045984. input_tokens=158, output_tokens=40
21:12:31,943 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:31,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.125. input_tokens=158, output_tokens=78
21:12:31,968 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:31,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2180000000007567. input_tokens=157, output_tokens=46
21:12:32,41 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.9060000000026776. input_tokens=168, output_tokens=43
21:12:32,54 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.937000000005355. input_tokens=215, output_tokens=133
21:12:32,72 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5160000000032596. input_tokens=142, output_tokens=35
21:12:32,168 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.109000000004016. input_tokens=186, output_tokens=55
21:12:32,235 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7339999999967404. input_tokens=198, output_tokens=87
21:12:32,729 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.5939999999973224. input_tokens=162, output_tokens=79
21:12:32,787 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8440000000045984. input_tokens=153, output_tokens=53
21:12:32,933 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:32,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3909999999959837. input_tokens=230, output_tokens=109
21:12:33,62 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:33,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2030000000013388. input_tokens=150, output_tokens=29
21:12:33,133 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:33,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.985000000000582. input_tokens=251, output_tokens=120
21:12:33,311 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:33,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3909999999959837. input_tokens=159, output_tokens=44
21:12:33,382 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:33,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9069999999992433. input_tokens=239, output_tokens=97
21:12:33,511 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:33,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.0939999999973224. input_tokens=189, output_tokens=74
21:12:34,157 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6410000000032596. input_tokens=219, output_tokens=87
21:12:34,361 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.390999999995984. input_tokens=189, output_tokens=96
21:12:34,367 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2039999999979045. input_tokens=204, output_tokens=72
21:12:34,405 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.328000000001339. input_tokens=366, output_tokens=238
21:12:34,487 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.125. input_tokens=189, output_tokens=71
21:12:34,494 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.89100000000326. input_tokens=197, output_tokens=105
21:12:34,662 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.328000000001339. input_tokens=264, output_tokens=145
21:12:34,690 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5619999999980791. input_tokens=170, output_tokens=42
21:12:34,742 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9219999999986612. input_tokens=157, output_tokens=56
21:12:34,803 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2180000000007567. input_tokens=202, output_tokens=67
21:12:34,918 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.5. input_tokens=160, output_tokens=53
21:12:34,985 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:34,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.937999999994645. input_tokens=222, output_tokens=55
21:12:35,47 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:35,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1090000000040163. input_tokens=189, output_tokens=77
21:12:35,279 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:35,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.172000000005937. input_tokens=187, output_tokens=106
21:12:35,312 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:35,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2339999999967404. input_tokens=230, output_tokens=64
21:12:35,435 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:35,436 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5. input_tokens=169, output_tokens=48
21:12:35,777 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:35,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2660000000032596. input_tokens=188, output_tokens=62
21:12:35,882 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:35,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.813000000001921. input_tokens=224, output_tokens=73
21:12:35,948 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:35,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2190000000045984. input_tokens=247, output_tokens=93
21:12:36,769 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:36,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4530000000013388. input_tokens=166, output_tokens=58
21:12:36,838 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:36,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.875. input_tokens=154, output_tokens=75
21:12:37,20 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:37,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6560000000026776. input_tokens=187, output_tokens=46
21:12:37,98 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:37,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5930000000007567. input_tokens=242, output_tokens=79
21:12:37,273 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:37,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.48399999999674. input_tokens=229, output_tokens=125
21:12:37,406 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:37,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.485000000000582. input_tokens=170, output_tokens=44
21:12:37,412 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:37,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.375. input_tokens=215, output_tokens=92
21:12:37,430 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:37,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7340000000040163. input_tokens=178, output_tokens=53
21:12:37,574 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:37,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5319999999992433. input_tokens=223, output_tokens=76
21:12:38,182 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:38,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.953000000001339. input_tokens=212, output_tokens=88
21:12:38,392 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:38,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.23399999999674. input_tokens=202, output_tokens=91
21:12:38,924 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:38,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4840000000040163. input_tokens=183, output_tokens=50
21:12:38,930 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:38,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.437000000005355. input_tokens=195, output_tokens=67
21:12:38,947 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:38,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6719999999986612. input_tokens=168, output_tokens=31
21:12:39,149 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5619999999980791. input_tokens=173, output_tokens=35
21:12:39,209 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8910000000032596. input_tokens=211, output_tokens=86
21:12:39,247 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.4539999999979045. input_tokens=198, output_tokens=66
21:12:39,394 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.639999999999418. input_tokens=245, output_tokens=118
21:12:39,450 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.796999999998661. input_tokens=232, output_tokens=91
21:12:39,570 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.186999999998079. input_tokens=218, output_tokens=71
21:12:39,600 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3280000000013388. input_tokens=190, output_tokens=52
21:12:39,922 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:39,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.484000000004016. input_tokens=287, output_tokens=213
21:12:40,107 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:40,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.155999999995402. input_tokens=241, output_tokens=67
21:12:40,152 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:40,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.735000000000582. input_tokens=199, output_tokens=52
21:12:40,178 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:40,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0780000000013388. input_tokens=273, output_tokens=76
21:12:40,395 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:40,396 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0. input_tokens=143, output_tokens=44
21:12:40,530 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:40,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.64100000000326. input_tokens=202, output_tokens=84
21:12:40,885 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:40,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.125. input_tokens=283, output_tokens=95
21:12:40,966 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:40,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.514999999999418. input_tokens=185, output_tokens=44
21:12:41,304 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:41,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0460000000020955. input_tokens=163, output_tokens=41
21:12:41,367 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:41,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.437999999994645. input_tokens=153, output_tokens=48
21:12:41,470 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:41,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.264999999999418. input_tokens=167, output_tokens=65
21:12:41,577 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:41,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0160000000032596. input_tokens=173, output_tokens=53
21:12:41,645 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:41,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.23399999999674. input_tokens=226, output_tokens=134
21:12:41,760 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:41,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.35899999999674. input_tokens=203, output_tokens=54
21:12:42,233 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:42,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.452999999994063. input_tokens=228, output_tokens=109
21:12:42,360 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:42,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.437999999994645. input_tokens=159, output_tokens=72
21:12:42,408 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:42,409 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.031000000002678. input_tokens=230, output_tokens=93
21:12:42,569 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:42,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1719999999986612. input_tokens=172, output_tokens=40
21:12:42,654 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:42,655 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5160000000032596. input_tokens=168, output_tokens=70
21:12:42,672 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:42,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.062000000005355. input_tokens=186, output_tokens=63
21:12:43,118 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:43,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.235000000000582. input_tokens=176, output_tokens=65
21:12:43,144 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:43,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.186999999998079. input_tokens=169, output_tokens=64
21:12:43,205 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:43,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2819999999992433. input_tokens=215, output_tokens=61
21:12:43,582 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:43,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.735000000000582. input_tokens=201, output_tokens=72
21:12:43,715 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:43,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.561999999998079. input_tokens=220, output_tokens=51
21:12:44,84 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.063000000001921. input_tokens=192, output_tokens=77
21:12:44,299 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.125. input_tokens=200, output_tokens=97
21:12:44,315 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7339999999967404. input_tokens=249, output_tokens=87
21:12:44,385 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.2810000000026776. input_tokens=214, output_tokens=47
21:12:44,460 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.485000000000582. input_tokens=163, output_tokens=39
21:12:44,521 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0469999999986612. input_tokens=255, output_tokens=66
21:12:44,772 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0. input_tokens=215, output_tokens=102
21:12:44,942 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:44,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2969999999986612. input_tokens=233, output_tokens=61
21:12:45,54 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:45,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4840000000040163. input_tokens=224, output_tokens=53
21:12:45,138 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:45,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.953000000001339. input_tokens=166, output_tokens=83
21:12:45,196 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:45,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5159999999959837. input_tokens=281, output_tokens=51
21:12:45,436 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:45,438 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.453000000001339. input_tokens=269, output_tokens=127
21:12:45,597 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:45,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.218000000000757. input_tokens=253, output_tokens=80
21:12:45,706 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:45,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5780000000013388. input_tokens=248, output_tokens=69
21:12:45,842 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:45,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.639999999999418. input_tokens=246, output_tokens=86
21:12:46,151 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:46,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5780000000013388. input_tokens=300, output_tokens=74
21:12:46,267 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:46,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9060000000026776. input_tokens=221, output_tokens=96
21:12:46,486 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:46,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.827999999994063. input_tokens=267, output_tokens=87
21:12:46,550 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:46,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8280000000013388. input_tokens=311, output_tokens=82
21:12:46,713 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:46,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.296999999998661. input_tokens=240, output_tokens=75
21:12:47,29 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:47,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.5. input_tokens=225, output_tokens=78
21:12:47,313 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:47,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.171999999998661. input_tokens=265, output_tokens=97
21:12:48,725 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:48,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.328000000001339. input_tokens=264, output_tokens=116
21:12:50,148 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:50,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.860000000000582. input_tokens=249, output_tokens=70
21:12:50,582 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:12:50,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.344000000004598. input_tokens=265, output_tokens=156
21:12:51,165 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:12:51,166 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
21:12:51,174 datashaper.workflow.workflow INFO executing verb create_final_entities
21:12:51,343 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:12:51,694 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:12:51,695 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
21:12:51,707 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:12:52,646 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:12:52,920 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:12:52,921 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
21:12:52,937 datashaper.workflow.workflow INFO executing verb create_final_communities
21:12:53,310 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:12:53,624 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:12:53,625 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:12:53,684 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
21:12:53,696 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:12:53,858 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:12:54,152 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
21:12:54,153 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:12:54,160 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
21:12:54,161 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:12:54,184 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:12:54,215 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:12:54,541 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_communities', 'create_final_nodes', 'create_final_relationships']
21:12:54,543 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
21:12:54,550 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:12:54,562 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:12:54,583 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:12:54,613 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=2 => 10
21:12:54,644 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 126
21:12:54,761 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 558
21:13:15,167 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:15,169 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:15,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.32800000000134. input_tokens=3021, output_tokens=780
21:13:18,828 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:18,829 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:18,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.9690000000046. input_tokens=2403, output_tokens=643
21:13:21,582 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:21,585 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:21,586 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'str'>:
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
21:13:21,590 graphrag.llm.openai.openai_chat_llm WARNING error parsing llm json, retrying
21:13:31,997 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:31,998 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:32,0 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'list'>:
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 6 (char 5)
21:13:35,899 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:35,901 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:35,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.875. input_tokens=2144, output_tokens=871
21:13:36,691 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:36,693 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:36,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.718999999997322. input_tokens=2212, output_tokens=612
21:13:37,558 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:37,559 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:37,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 18.56199999999808. input_tokens=2667, output_tokens=644
21:13:39,26 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:39,27 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:39,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.0940000000046. input_tokens=2205, output_tokens=548
21:13:41,237 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:41,239 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:41,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 22.312999999994645. input_tokens=2375, output_tokens=722
21:13:42,272 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:42,273 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:42,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.32800000000134. input_tokens=2336, output_tokens=661
21:13:42,609 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:42,611 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:42,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.6559999999954. input_tokens=2144, output_tokens=530
21:13:43,717 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:43,719 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:43,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.764999999999418. input_tokens=3526, output_tokens=753
21:13:44,821 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:44,823 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:44,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.890999999995984. input_tokens=2989, output_tokens=647
21:13:45,504 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:45,506 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:45,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.51600000000326. input_tokens=2437, output_tokens=639
21:13:46,832 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:46,835 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:46,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.95300000000134. input_tokens=3832, output_tokens=709
21:13:47,603 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:47,605 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:47,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.718999999997322. input_tokens=2345, output_tokens=611
21:13:47,660 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:47,661 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:47,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.68800000000192. input_tokens=2613, output_tokens=576
21:13:47,682 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:47,683 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:47,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.79699999999866. input_tokens=2621, output_tokens=708
21:13:48,46 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:48,49 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:48,50 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.031000000002678. input_tokens=2508, output_tokens=780
21:13:48,436 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:48,438 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:48,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.48399999999674. input_tokens=2114, output_tokens=657
21:13:49,171 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:49,173 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:49,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.171000000002095. input_tokens=2711, output_tokens=782
21:13:52,550 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:52,552 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:52,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.63999999999942. input_tokens=2598, output_tokens=781
21:13:53,675 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:53,676 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:53,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.796000000002095. input_tokens=2264, output_tokens=643
21:13:54,258 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:54,261 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:54,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.35899999999674. input_tokens=2868, output_tokens=858
21:13:54,447 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:54,448 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:54,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.53199999999924. input_tokens=3971, output_tokens=772
21:13:55,217 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:55,219 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:55,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.31199999999808. input_tokens=2356, output_tokens=568
21:13:55,457 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:55,459 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:55,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.56300000000192. input_tokens=2905, output_tokens=779
21:13:56,202 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:56,204 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:56,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.20300000000134. input_tokens=2315, output_tokens=600
21:13:56,884 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:13:56,885 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:13:56,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.90699999999924. input_tokens=2503, output_tokens=786
21:14:00,47 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:00,50 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:00,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.359000000004016. input_tokens=2347, output_tokens=596
21:14:01,196 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:01,198 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:01,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 23.64100000000326. input_tokens=2639, output_tokens=772
21:14:02,827 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:02,828 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:02,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.8440000000046. input_tokens=2539, output_tokens=762
21:14:09,301 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:09,303 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:09,303 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'str'>:
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
21:14:09,304 graphrag.llm.openai.openai_chat_llm WARNING error parsing llm json, retrying
21:14:24,731 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:24,733 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:24,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.827999999994063. input_tokens=3196, output_tokens=735
21:14:25,875 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:25,876 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:25,877 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'list'>:
Traceback (most recent call last):
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\site-packages\graphrag\llm\openai\utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\anaconda3\envs\graph\Lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 6 (char 5)
21:14:28,780 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:28,783 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:28,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.93800000000192. input_tokens=2530, output_tokens=662
21:14:29,191 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:29,192 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:29,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.32800000000134. input_tokens=3395, output_tokens=775
21:14:31,695 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:31,696 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:31,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.81199999999808. input_tokens=4633, output_tokens=865
21:14:36,334 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:36,337 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:36,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.42199999999866. input_tokens=3459, output_tokens=808
21:14:36,696 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:36,698 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:36,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.8440000000046. input_tokens=2782, output_tokens=710
21:14:37,267 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:37,270 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:37,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.34399999999732. input_tokens=2914, output_tokens=687
21:14:42,938 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:42,940 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:42,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.04699999999866. input_tokens=5153, output_tokens=855
21:14:44,600 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:44,602 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:44,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.71800000000076. input_tokens=5298, output_tokens=1111
21:14:45,498 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:14:45,499 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:14:45,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 42.59399999999732. input_tokens=4558, output_tokens=913
21:15:00,436 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/chat/completions "HTTP/1.1 200 OK"
21:15:00,438 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
21:15:00,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 57.54699999999866. input_tokens=4747, output_tokens=795
21:15:00,456 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:15:00,801 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:15:00,802 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:15:00,825 datashaper.workflow.workflow INFO executing verb create_final_documents
21:15:00,843 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:15:01,98 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_entities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:15:01,98 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:15:01,106 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:15:01,122 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:15:01,130 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
21:15:01,139 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
21:15:01,170 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
21:15:01,174 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
21:15:01,174 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
21:15:01,189 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://open.bigmodel.cn/api/paas/v4
21:15:01,509 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-3: TPM=0, RPM=0
21:15:01,510 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-3: 25
21:15:01,576 graphrag.index.operations.embed_text.strategies.openai INFO embedding 61 inputs via 61 snippets using 10 batches. max_batch_size=16, max_tokens=8191
21:15:01,976 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:01,998 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,7 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,59 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,60 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5470000000059372. input_tokens=7200, output_tokens=0
21:15:02,188 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,190 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,191 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6560000000026776. input_tokens=4930, output_tokens=0
21:15:02,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7819999999992433. input_tokens=7398, output_tokens=0
21:15:02,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8899999999994179. input_tokens=7199, output_tokens=0
21:15:02,479 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,481 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:02,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9530000000013388. input_tokens=7200, output_tokens=0
21:15:02,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0930000000007567. input_tokens=7749, output_tokens=0
21:15:02,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1719999999986612. input_tokens=7199, output_tokens=0
21:15:02,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2659999999959837. input_tokens=7199, output_tokens=0
21:15:02,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.375. input_tokens=7200, output_tokens=0
21:15:03,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4689999999973224. input_tokens=7199, output_tokens=0
21:15:03,464 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
21:15:03,500 graphrag.index.operations.embed_text.strategies.openai INFO embedding 500 inputs via 500 snippets using 32 batches. max_batch_size=16, max_tokens=8191
21:15:03,845 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,854 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,862 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,902 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,904 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,906 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,913 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,925 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,934 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,967 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,967 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,969 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:03,975 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7189999999973224. input_tokens=368, output_tokens=0
21:15:04,274 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,275 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,278 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,280 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,281 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,283 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,286 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,287 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,288 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:04,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9850000000005821. input_tokens=451, output_tokens=0
21:15:04,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.235000000000582. input_tokens=532, output_tokens=0
21:15:05,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.514999999999418. input_tokens=543, output_tokens=0
21:15:05,104 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:05,105 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:05,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8280000000013388. input_tokens=680, output_tokens=0
21:15:05,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0469999999986612. input_tokens=454, output_tokens=0
21:15:05,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.2819999999992433. input_tokens=378, output_tokens=0
21:15:06,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.5469999999986612. input_tokens=786, output_tokens=0
21:15:06,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.7969999999986612. input_tokens=397, output_tokens=0
21:15:06,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.0469999999986612. input_tokens=618, output_tokens=0
21:15:06,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.2810000000026776. input_tokens=834, output_tokens=0
21:15:07,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.561999999998079. input_tokens=587, output_tokens=0
21:15:07,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.7810000000026776. input_tokens=592, output_tokens=0
21:15:07,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.078000000001339. input_tokens=464, output_tokens=0
21:15:07,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.311999999998079. input_tokens=921, output_tokens=0
21:15:08,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.531999999999243. input_tokens=428, output_tokens=0
21:15:08,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.813000000001921. input_tokens=460, output_tokens=0
21:15:08,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.046999999998661. input_tokens=517, output_tokens=0
21:15:08,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.296999999998661. input_tokens=640, output_tokens=0
21:15:09,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.515999999995984. input_tokens=758, output_tokens=0
21:15:09,103 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:09,104 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:09,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.827999999994063. input_tokens=430, output_tokens=0
21:15:09,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.10899999999674. input_tokens=744, output_tokens=0
21:15:09,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.360000000000582. input_tokens=675, output_tokens=0
21:15:09,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.703000000001339. input_tokens=201, output_tokens=0
21:15:10,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.640999999995984. input_tokens=931, output_tokens=0
21:15:10,226 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:10,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.9689999999973224. input_tokens=349, output_tokens=0
21:15:10,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.953000000001339. input_tokens=669, output_tokens=0
21:15:10,784 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:11,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.6560000000026776. input_tokens=395, output_tokens=0
21:15:11,58 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:11,60 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:11,81 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:11,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.514999999999418. input_tokens=1287, output_tokens=0
21:15:11,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 4.5. input_tokens=508, output_tokens=0
21:15:11,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 6.764999999999418. input_tokens=612, output_tokens=0
21:15:11,883 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:12,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.811999999998079. input_tokens=576, output_tokens=0
21:15:12,331 graphrag.index.operations.embed_text.strategies.openai INFO embedding 58 inputs via 58 snippets using 4 batches. max_batch_size=16, max_tokens=8191
21:15:12,502 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:12,559 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:12,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.375. input_tokens=239, output_tokens=0
21:15:12,714 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:12,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.625. input_tokens=311, output_tokens=0
21:15:12,997 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:13,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8909999999959837. input_tokens=280, output_tokens=0
21:15:13,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1869999999980791. input_tokens=327, output_tokens=0
21:15:13,597 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
21:15:13,640 graphrag.index.operations.embed_text.strategies.openai INFO embedding 41 inputs via 41 snippets using 3 batches. max_batch_size=16, max_tokens=8191
21:15:14,75 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:14,79 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:14,150 httpx INFO HTTP Request: POST https://open.bigmodel.cn/api/paas/v4/embeddings "HTTP/1.1 200 OK"
21:15:14,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6869999999980791. input_tokens=7601, output_tokens=0
21:15:14,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9219999999986612. input_tokens=7791, output_tokens=0
21:15:14,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1560000000026776. input_tokens=7638, output_tokens=0
21:15:14,917 graphrag.cli.index INFO All workflows completed successfully.
